diff -ruN c/Makefile d/Makefile
--- c/Makefile	2015-08-04 16:50:22.300617726 +0900
+++ d/Makefile	2015-09-03 01:30:54.674380525 +0900
@@ -1,7 +1,7 @@
 VERSION = 3
 PATCHLEVEL = 14
 SUBLEVEL = 33
-EXTRAVERSION =
+EXTRAVERSION = -el-magw-v02
 NAME = Remembering Coco
 
 # *DOCUMENTATION*
diff -ruN c/include/linux/tcp.h d/include/linux/tcp.h
--- c/include/linux/tcp.h	2015-08-04 16:50:26.626558940 +0900
+++ d/include/linux/tcp.h	2015-09-08 17:27:41.462083544 +0900
@@ -397,6 +397,11 @@
 	u32		mptcp_loc_token;
 	u64		mptcp_loc_key;
 #endif /* CONFIG_MPTCP */
+#ifdef CONFIG_NC_KT_MAGW /* 0907 */
+	u8		netProto;
+	u8		intfNum;
+	u8		ncPadd[2];
+#endif /* CONFIG_NC_KT_MAGW */
 
 	/* Functions that depend on the value of the mpc flag */
 	u32 (*__select_window)(struct sock *sk);
diff -ruN c/include/net/mptcp.h d/include/net/mptcp.h
--- c/include/net/mptcp.h	2015-08-04 16:50:26.671558329 +0900
+++ d/include/net/mptcp.h	2015-09-18 23:42:03.445442439 +0900
@@ -45,6 +45,11 @@
 #include <crypto/hash.h>
 #include <net/tcp.h>
 
+
+#ifdef CONFIG_NC_KT_MAGW
+#include <net/mptcp_magw.h>
+#endif /* CONFIG_NC_KT_MAGW */
+
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	#define ntohll(x)  be64_to_cpu(x)
 	#define htonll(x)  cpu_to_be64(x)
@@ -204,6 +209,14 @@
 
 	/* HMAC of the third ack */
 	char sender_mac[20];
+#ifdef CONFIG_NC_KT_MAGW
+	struct {
+		u64	rxPkts;
+		u64	rxOctets;
+		u64	txPkts;
+		u64	txOctets;
+	} nc_stat[1];
+#endif
 };
 
 struct mptcp_tw {
@@ -239,8 +252,13 @@
 };
 
 #define MPTCP_SCHED_NAME_MAX 16
+#define MPTCP_SCHED_MAX  32 //0918
+#define MPTCP_SCHED_BUF_MAX (MPTCP_SCHED_NAME_MAX*MPTCP_SCHED_MAX) //0918
+
+#define MPTCP_SCHED_NON_RESTRICTED 0x1
 struct mptcp_sched_ops {
 	struct list_head list;
+	unsigned long flags; //0918
 
 	struct sock *		(*get_subflow)(struct sock *meta_sk,
 					       struct sk_buff *skb,
@@ -255,6 +273,7 @@
 	struct module		*owner;
 };
 
+
 struct mptcp_cb {
 	struct sock *meta_sk;
 
@@ -299,6 +318,11 @@
 
 	struct mptcp_sched_ops *sched_ops;
 
+#ifdef CONFIG_NC_KT_MAGW
+	u8	isReportDestroy;
+	u8	ncPadd[3];
+	struct mptcp_sm_ops *sm_ops;
+#endif
 	/* Mutex needed, because otherwise mptcp_close will complain that the
 	 * socket is owned by the user.
 	 * E.g., mptcp_sub_close_wq is taking the meta-lock.
@@ -665,6 +689,9 @@
 extern int sysctl_mptcp_enabled;
 extern int sysctl_mptcp_checksum;
 extern int sysctl_mptcp_debug;
+extern int sysctl_mptcp_gw_port; //0907
+extern u16 mptcp_gw_port; //0907
+extern int sysctl_mptcp_sm_debug;
 extern int sysctl_mptcp_syn_retries;
 
 extern struct workqueue_struct *mptcp_wq;
@@ -698,6 +725,67 @@
 #define mptcp_for_each_bit_unset(b, i)					\
 	mptcp_for_each_bit_set(~b, i)
 
+#define MPTCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mptcp.mptcp_statistics, field)
+#define MPTCP_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mptcp.mptcp_statistics, field)
+#define MPTCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mptcp.mptcp_statistics, field)
+#define MPTCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mptcp.mptcp_statistics, field, val)
+#define MPTCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mptcp.mptcp_statistics, field, val)
+
+enum
+{
+	MPTCP_MIB_NUM = 0,
+	MPTCP_MIB_MPCAPABLEPASSIVE,	/* Received SYN with MP_CAPABLE */
+	MPTCP_MIB_MPCAPABLEACTIVE,	/* Sent SYN with MP_CAPABLE */
+	MPTCP_MIB_MPCAPABLEACTIVEACK,	/* Received SYN/ACK with MP_CAPABLE */
+	MPTCP_MIB_MPCAPABLEPASSIVEACK,	/* Received third ACK with MP_CAPABLE */
+	MPTCP_MIB_MPCAPABLEPASSIVEFALLBACK,/* Server-side fallback during 3-way handshake */
+	MPTCP_MIB_MPCAPABLEACTIVEFALLBACK, /* Client-side fallback during 3-way handshake */
+	MPTCP_MIB_MPCAPABLERETRANSFALLBACK,/* Client-side stopped sending MP_CAPABLE after too many SYN-retransmissions */
+	MPTCP_MIB_CSUMENABLED,		/* Created MPTCP-connection with DSS-checksum enabled */
+	MPTCP_MIB_RETRANSSEGS,		/* Segments retransmitted at the MPTCP-level */
+	MPTCP_MIB_MPFAILRX,		/* Received an MP_FAIL */
+	MPTCP_MIB_CSUMFAIL,		/* Received segment with invalid checksum */
+	MPTCP_MIB_FASTCLOSERX,		/* Recevied a FAST_CLOSE */
+	MPTCP_MIB_FASTCLOSETX,		/* Sent a FAST_CLOSE */
+	MPTCP_MIB_FBACKSUB,		/* Fallback upon ack without data-ack on new subflow */
+	MPTCP_MIB_FBACKINIT,		/* Fallback upon ack without data-ack on initial subflow */
+	MPTCP_MIB_FBDATASUB,		/* Fallback upon data without DSS at the beginning on new subflow */
+	MPTCP_MIB_FBDATAINIT,		/* Fallback upon data without DSS at the beginning on initial subflow */
+	MPTCP_MIB_REMADDRSUB,		/* Remove subflow due to REMOVE_ADDR */
+	MPTCP_MIB_JOINNOTOKEN,		/* Received MP_JOIN but the token was not found */
+	MPTCP_MIB_JOINFALLBACK,		/* Received MP_JOIN on session that has fallen back to reg. TCP */
+	MPTCP_MIB_JOINSYNTX,		/* Sent a SYN + MP_JOIN */
+	MPTCP_MIB_JOINSYNRX,		/* Received a SYN + MP_JOIN */
+	MPTCP_MIB_JOINSYNACKRX,		/* Received a SYN/ACK + MP_JOIN */
+	MPTCP_MIB_JOINSYNACKMAC,	/* HMAC was wrong on SYN/ACK + MP_JOIN */
+	MPTCP_MIB_JOINACKRX,		/* Received an ACK + MP_JOIN */
+	MPTCP_MIB_JOINACKMAC,		/* HMAC was wrong on ACK + MP_JOIN */
+	MPTCP_MIB_JOINACKFAIL,		/* Third ACK on new subflow did not contain an MP_JOIN */
+	MPTCP_MIB_JOINACKRTO,		/* Retransmission timer for third ACK + MP_JOIN timed out */
+	MPTCP_MIB_JOINACKRXMIT,		/* Retransmitted an ACK + MP_JOIN */
+	MPTCP_MIB_NODSSWINDOW,		/* Received too many packets without a DSS-option */
+	MPTCP_MIB_DSSNOMATCH,		/* Received a new mapping that did not match the previous one */
+	MPTCP_MIB_INFINITEMAPRX,	/* Received an infinite mapping */
+	MPTCP_MIB_DSSTCPMISMATCH,	/* DSS-mapping did not map with TCP's sequence numbers */
+	MPTCP_MIB_DSSTRIMHEAD,		/* Trimmed segment at the head (coalescing middlebox) */
+	MPTCP_MIB_DSSSPLITTAIL,		/* Trimmed segment at the tail (coalescing middlebox) */
+	MPTCP_MIB_PURGEOLD,		/* Removed old skb from the rcv-queue due to missing DSS-mapping */
+	MPTCP_MIB_ADDADDRRX,		/* Received an ADD_ADDR */
+	MPTCP_MIB_ADDADDRTX,		/* Sent an ADD_ADDR */
+	MPTCP_MIB_REMADDRRX,		/* Received a REMOVE_ADDR */
+	MPTCP_MIB_REMADDRTX,		/* Sent a REMOVE_ADDR */
+#ifdef CONFIG_NC_KT_MAGW
+	MPTCP_MIB_DATAFINRX,		/* Received a DATA_FIN */
+	MPTCP_MIB_DATAFINTX,		/* Sent a DATA_FIN  */
+#endif
+	__MPTCP_MIB_MAX
+};
+
+#define MPTCP_MIB_MAX __MPTCP_MIB_MAX
+struct mptcp_mib {
+	unsigned long	mibs[MPTCP_MIB_MAX];
+};
+
 extern struct lock_class_key meta_key;
 extern struct lock_class_key meta_slock_key;
 extern u32 mptcp_secret[MD5_MESSAGE_BYTES / 4];
@@ -860,8 +948,16 @@
 void mptcp_cleanup_scheduler(struct mptcp_cb *mpcb);
 void mptcp_get_default_scheduler(char *name);
 int mptcp_set_default_scheduler(const char *name);
+int mptcp_set_scheduler(struct sock *sk, const char *name);
+//int mptcp_set_allowed_scheduler(char *val);
+int mptcp_set_allowed_scheduler(char *allowed);
+void mptcp_get_allowed_scheduler(char *buf, size_t maxlen);
+void mptcp_get_available_scheduler(char *buf, size_t maxlen);
 extern struct mptcp_sched_ops mptcp_sched_default;
 
+
+void mptcp_get_available_scheduler(char *buf, size_t maxlen);
+
 /* Initializes function-pointers and MPTCP-flags */
 static inline void mptcp_init_tcp_sock(struct sock *sk)
 {
@@ -1247,8 +1343,10 @@
 	       __func__, tp->mpcb->mptcp_loc_token, tp->mptcp->path_index,
 	       &inet_sk(sk)->inet_saddr, &inet_sk(sk)->inet_daddr,
 	       __builtin_return_address(0));
-	if (!is_master_tp(tp))
+	if (!is_master_tp(tp)) {
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_FBACKSUB);
 		return true;
+	}
 
 	tp->mpcb->infinite_mapping_snd = 1;
 	tp->mpcb->infinite_mapping_rcv = 1;
@@ -1256,6 +1354,8 @@
 
 	mptcp_sub_force_close_all(tp->mpcb, sk);
 
+	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_FBACKINIT);
+
 	return false;
 }
 
@@ -1357,6 +1457,8 @@
 }
 
 #else /* CONFIG_MPTCP */
+
+
 #define mptcp_debug(fmt, args...)	\
 	do {				\
 	} while (0)
@@ -1368,6 +1470,10 @@
 #define mptcp_for_each_sk(mpcb, sk)
 #define mptcp_for_each_sk_safe(__mpcb, __sk, __temp)
 
+#define MPTCP_INC_STATS(net, field)	\
+	do {				\
+	} while(0)
+
 static inline bool mptcp_is_data_fin(const struct sk_buff *skb)
 {
 	return 0;
diff -ruN c/include/net/mptcp_magw.h d/include/net/mptcp_magw.h
--- c/include/net/mptcp_magw.h	1970-01-01 09:00:00.000000000 +0900
+++ d/include/net/mptcp_magw.h	2015-09-19 17:05:38.763285673 +0900
@@ -0,0 +1,242 @@
+/*
+ *	MPTCP MAGW implementation
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _MPTCP_MAGW_H
+#define _MPTCP_MAGW_H
+
+#include <linux/inetdevice.h>
+#include <linux/ipv6.h>
+#include <linux/list.h>
+#include <linux/net.h>
+#include <linux/netpoll.h>
+#include <linux/skbuff.h>
+#include <linux/socket.h>
+#include <linux/tcp.h>
+#include <linux/kernel.h>
+
+#include <asm/byteorder.h>
+#include <asm/unaligned.h>
+#include <crypto/hash.h>
+#include <net/tcp.h>
+#include <net/genetlink.h>
+
+
+/* ------------------------------------------------------------------------- */
+/*  For Session Monitor                                                      */
+/* ------------------------------------------------------------------------- */
+#define MPTCP_SM_NAME_MAX 16
+#define MPTCP_SM_ADDR_MAX 64
+
+struct mptcp_sm_ops {
+	struct list_head list;
+	void (*on_new_master_session)(struct mptcp_cb *mpcb,
+			                      struct sock *sk);
+	void (*on_new_sub_session)   (struct mptcp_cb *mpcb,
+			                      struct sock *sk);
+	void (*on_del_session)       (struct mptcp_cb *mpcb,
+			                      struct sock *sk);
+	void (*on_destory_session)   (struct mptcp_cb *mpcb, 
+			                      struct sock *sk);
+	char			name[MPTCP_SM_NAME_MAX];
+	struct module		*owner;
+};
+
+/* ------------------------------------------------------------------------- */
+/*  For UE IP Range                                                          */
+/* ------------------------------------------------------------------------- */
+typedef struct _range_4_lte {
+    uint32_t    used;
+    uint32_t    s;
+    uint32_t    e;
+} R4L;
+
+typedef struct _range_tb {
+    int         cnt;
+#define MAX_RTB_ROWS    200
+    R4L         r[MAX_RTB_ROWS];
+} RTB;
+
+typedef struct _lf_rtb { /*lock free Range Table*/
+    int         idx;
+    RTB         tb[2];
+} lfRTB;
+
+/* ------------------------------------------------------------------------- */
+/*  For GW IP                                                                */
+/* ------------------------------------------------------------------------- */
+typedef struct _nc_gw_tb {
+#define MAX_GWTB_ROWS    2
+	__be32   		  addr4[MAX_GWTB_ROWS];
+	struct in6_addr   addr6[MAX_GWTB_ROWS];
+} GwTB;
+
+typedef struct _lf_gwtb {
+    int         idx;
+	GwTB		tb[2];
+} lfGwTB;
+
+
+/* ------------------------------------------------------------------------- */
+/*  For GW specific MPTCP Statistic                                          */
+/* ------------------------------------------------------------------------- */
+#define MAGWS_INC(net,_f,_i,_j,_k, field)   do{\
+ if(_f) {                                      \
+      SNMP_INC_STATS((net)->mptcp.mg_stat[_i][_j][_k], field);\
+ }                                             \
+}while(0)
+
+#if 0
+#define MAGW_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mptcp.mptcp_statistics, field)
+#define MAGW_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mptcp.mptcp_statistics, field)
+#define MAGW_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mptcp.mptcp_statistics, field, val)
+#define MAGW_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mptcp.mptcp_statistics, field, val)
+#endif
+
+enum {
+	MAGW_MIB_NUM = 0,
+ 	MAGW_MIB_MCSYN_RX,
+ 	MAGW_MIB_MCSACK_TX,
+ 	MAGW_MIB_MCACK_RX,
+ 	MAGW_MIB_MJSYN_RX,
+ 	MAGW_MIB_MJSACK_TX,
+ 	MAGW_MIB_MJACK_RX,
+ 	MAGW_MIB_FAIL_TX,
+ 	MAGW_MIB_FAIL_RX,
+ 	MAGW_MIB_FCLOSE_TX,
+ 	MAGW_MIB_FCLOSE_RX,
+ 	MAGW_MIB_DTFIN_TX,
+ 	MAGW_MIB_DTFIN_RX,
+ 	MAGW_MIB_RMADDR_TX,
+ 	MAGW_MIB_RMADDR_RX,
+ 	MAGW_MIB_ADADDR_TX,
+ 	MAGW_MIB_ADADDR_RX,
+ /* MPTCP Fail */
+ 	MAGW_MIB_MP_TRY,
+ 	MAGW_MIB_MP_SUCCESS,
+ 	MAGW_MIB_MP_FAIL,
+ 	MAGW_MIB_MP_CYE,/* MP_CAPABLE SYN ERROR */
+ 	MAGW_MIB_MP_CTE,/* MP_CAPABLE TIMEOUT ERROR */
+ 	MAGW_MIB_MP_CAE,/* MP_CAPABLE ACK ERROR */
+ 	MAGW_MIB_MP_CCE,/* MP_CAPABLE CHECKSUM ERROR */
+ 	MAGW_MIB_MP_JSE,/* MP_JOIN SYN ERROR */
+ 	MAGW_MIB_MP_JTE,/* MP_JOIN TIMEOUT ERROR */
+ 	MAGW_MIB_MP_JAE,/* MP_JOIN ACK ERROR */
+ 	MAGW_MIB_MP_JCE,/* MP_JOIN SYN ERROR */
+	__MAGW_MIB_MAX
+};
+
+#define MAGW_MIB_MAX __MAGW_MIB_MAX
+struct magw_mib {
+	unsigned long	mibs[MAGW_MIB_MAX];
+};
+
+
+#ifdef CONFIG_NC_KT_MAGW
+
+#define NC_INC_RX(_st, _len)   do {\
+  if(_len>0) {/*0907*/            \
+    (_st)->rxPkts++;               \
+    (_st)->rxOctets += _len;       \
+  }                                \
+} while(0)
+#define NC_INC_TX(_st, _len)   do {\
+  if(_len>0) { /*0907*/           \
+    (_st)->txPkts++;               \
+    (_st)->txOctets += _len;       \
+  }                                \
+} while(0)
+
+#define mptcp_sm_debug(fmt, args...)  do {\
+  if (unlikely(sysctl_mptcp_sm_debug))	  \
+    pr_err(__FILE__ ": " fmt, ##args);	  \
+} while(0)
+
+#define magw_st_debug(fmt, args...)  do {\
+  if (unlikely(sysctl_magw_st_debug))	  \
+    pr_err(__FILE__ ": " fmt, ##args);	  \
+} while(0)
+
+
+/* MPTCP-session-monitor registration/initialization functions */
+int  mptcp_register_session_monitor(struct mptcp_sm_ops *sm);
+void mptcp_unregister_session_monitor(struct mptcp_sm_ops *sm);
+void mptcp_init_session_monitor(struct mptcp_cb *mpcb);
+void mptcp_cleanup_session_monitor(struct mptcp_cb *mpcb);
+void mptcp_sm_fallback_default(struct mptcp_cb *mpcb);
+void mptcp_get_default_session_monitor(char *name);
+int  mptcp_set_default_session_monitor(const char *name);
+
+/* MAGW CDR  */
+void nc_inc_rx_stat(struct tcp_sock *tp, u32 len);
+void nc_inc_tx_stat(struct tcp_sock *tp, u32 len);
+
+/* MPTCP Statistics  */
+void rSetRow       (int idx, u32 uS, u32 uE);
+int  isIPv4LTE     (uint32_t host);
+int  isIPv4MagwSvc (uint32_t host, u8 *idx);
+void lfRTBUpdate   (void *param);
+int  magw_proc_init_net(struct net *net);
+void magw_proc_exit_net(struct net *net);
+int  magw_mib_init_net (struct net *net);
+void magw_mib_exit_net (struct net *net);
+inline 
+void mgCheckStat(struct sock *sk, struct sk_buff *skb, u8 *vals, char *name);
+inline 
+void mgCheckStat2(struct sock *sk, struct sock *child, u8 *vals, char *name);
+inline 
+void mgCheckStat3(struct sk_buff *skb, u8 *vals, char *name);
+
+void gwSetIPv4(u8 idx, const char *addr, int len);
+void gwGetIPv4(u8 idx, char *addr, int len);
+
+void gwSetIPv6(u8 idx, const char *addr, int len);
+void gwGetIPv6(u8 idx, char *addr, int len);
+
+extern struct mptcp_sm_ops mptcp_sm_default;
+
+extern int sysctl_mptcp_gw_port; //0907
+extern u16 mptcp_gw_port; //0907
+extern int sysctl_mptcp_sm_debug;
+extern int sysctl_magw_st_debug;
+
+
+#else /* CONFIG_NC_KT_MAGW */
+
+
+static inline int  mptcp_register_session_monitor(struct mptcp_sm_ops *sm) {return 0;};
+static inline void mptcp_unregister_session_monitor(struct mptcp_sm_ops *sm) {};
+static inline void mptcp_init_session_monitor(struct mptcp_cb *mpcb) {};
+static inline void mptcp_cleanup_session_monitor(struct mptcp_cb *mpcb) {};
+static inline void mptcp_sm_fallback_default(struct mptcp_cb *mpcb) {};
+static inline void mptcp_get_default_session_monitor(char *name) {};
+static inline int  mptcp_set_default_session_monitor(const char *name) {return 0;};
+static inline void nc_inc_rx_stat(struct tcp_sock *tp, u32 len) {};
+static inline void nc_inc_tx_stat(struct tcp_sock *tp, u32 len) {};
+static inline void rSetRow(int idx, u32 uS, u32 uE) {};
+static inline int isIPv4LTE(uint32_t host) {return 0;};
+static inline int isIPv4MagwSvc(uint32_t host, u8 *idx) {return 0;};
+static inline void lfRTBUpdate (void *param) {};
+static inline int  magw_proc_init_net(struct net *net){return 0;};
+static inline void magw_proc_exit_net(struct net *net);
+static inline int  magw_mib_init_net(struct net *net){return 0;};
+static inline void magw_mib_exit_net(struct net *net);
+static inline void mgCheckStat(struct sock *sk, struct sk_buff *skb, u8 *vals, char *name){};
+static inline void mgCheckStat2(struct sock *sk, struct sock *child, u8 *vals, char *name){};
+static inline void mgCheckStat3(struct sk_buff *skb, u8 *vals, char *name){};
+#define mptcp_sm_debug(fmt, args...)	do {} while(0)
+#define magw_st_debug(fmt, args...)	do {} while(0)
+#define NC_INC_RX(_st, _len)            do {} while(0)
+#define NC_INC_TX(_st, _len)   			do {} while(0)
+#define MAGW_INC_STATS_I1_V4(net, field) do {} while(0)
+#define MAGW_INC_STATS_I2_V4(net, field) do {} while(0)
+#define MAGW_INC_STATS_I1_V6(net, field) do {} while(0)
+#define MAGW_INC_STATS_I2_V6(net, field) do {} while(0)
+
+#endif /* CONFIG_NC_KT_MAGW */
+
+#endif /* _MPTCP_MAGW_H */
diff -ruN c/include/net/netns/mptcp.h d/include/net/netns/mptcp.h
--- c/include/net/netns/mptcp.h	2015-08-04 16:50:26.676558261 +0900
+++ d/include/net/netns/mptcp.h	2015-09-11 21:06:01.759421679 +0900
@@ -37,7 +37,60 @@
 	MPTCP_PM_MAX
 };
 
+struct mptcp_mib;
+
+#ifdef CONFIG_NC_KT_MAGW
+struct magw_mib;
+enum {
+	MAGW_FM_IPV4 =0,
+	MAGW_FM_IPV6,
+	MAGW_FM_MAX
+};
+enum {
+	MAGW_IF_1 =0,
+	MAGW_IF_2,
+	MAGW_IF_MAX
+};
+enum {
+	MAGW_SVC_LTE =0,
+	MAGW_SVC_WIFI,
+	MAGW_SVC_MAX
+};
+#endif /* CONFIG_NC_KT_MAGW */
+
 struct netns_mptcp {
+	DEFINE_SNMP_STAT(struct mptcp_mib, mptcp_statistics);
+
+#ifdef CONFIG_NC_KT_MAGW
+#if 0
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i1v4L_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i1v4W_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i2v4L_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i2v4W_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i1v6L_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i1v6W_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i2v6L_stat);
+	DEFINE_SNMP_STAT(struct magw_mib, mg_i2v6W_stat);
+#else
+	DEFINE_SNMP_STAT(struct magw_mib, \
+			   mg_stat[MAGW_FM_MAX][MAGW_IF_MAX][MAGW_SVC_MAX]);
+
+#define mg_i1v4L_stat mg_stat[MAGW_FM_IPV4][MAGW_IF_1][MAGW_SVC_LTE]
+#define mg_i1v4W_stat mg_stat[MAGW_FM_IPV4][MAGW_IF_1][MAGW_SVC_WIFI]
+#define mg_i2v4L_stat mg_stat[MAGW_FM_IPV4][MAGW_IF_2][MAGW_SVC_LTE]
+#define mg_i2v4W_stat mg_stat[MAGW_FM_IPV4][MAGW_IF_2][MAGW_SVC_WIFI]
+#define mg_i1v6L_stat mg_stat[MAGW_FM_IPV6][MAGW_IF_1][MAGW_SVC_LTE]
+#define mg_i1v6W_stat mg_stat[MAGW_FM_IPV6][MAGW_IF_1][MAGW_SVC_WIFI]
+#define mg_i2v6L_stat mg_stat[MAGW_FM_IPV6][MAGW_IF_2][MAGW_SVC_LTE]
+#define mg_i2v6W_stat mg_stat[MAGW_FM_IPV6][MAGW_IF_2][MAGW_SVC_WIFI]
+
+#endif
+#endif /* CONFIG_NC_KT_MAGW */
+
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *proc_net_mptcp;
+#endif
+
 	void *path_managers[MPTCP_PM_MAX];
 };
 
diff -ruN c/include/net/snmp.h d/include/net/snmp.h
--- c/include/net/snmp.h	2015-08-04 16:50:26.681558193 +0900
+++ d/include/net/snmp.h	2015-08-04 17:58:51.054690602 +0900
@@ -91,7 +91,6 @@
 	atomic_long_t	mibs[ICMP6MSG_MIB_MAX];
 };
 
-
 /* TCP */
 #define TCP_MIB_MAX	__TCP_MIB_MAX
 struct tcp_mib {
Binary files c/include/uapi/linux/.snmp.h.swp and d/include/uapi/linux/.snmp.h.swp differ
diff -ruN c/include/uapi/linux/tcp.h d/include/uapi/linux/tcp.h
--- c/include/uapi/linux/tcp.h	2015-08-04 16:50:26.759557133 +0900
+++ d/include/uapi/linux/tcp.h	2015-09-18 23:53:15.750654092 +0900
@@ -113,6 +113,8 @@
 #define TCP_TIMESTAMP		24
 #define TCP_NOTSENT_LOWAT	25	/* limit number of unsent bytes in write queue */
 #define MPTCP_ENABLED		26
+#define MAGW_NETPROTO		27 /* CONFIG_NC_KT_MAGW  0907 */
+#define MPTCP_SCHEDULER		28 /* CONFIG_NC_KT_MAGW  0917 */
 
 struct tcp_repair_opt {
 	__u32	opt_code;
diff -ruN c/make.sh d/make.sh
--- c/make.sh	1970-01-01 09:00:00.000000000 +0900
+++ d/make.sh	2015-09-25 00:46:58.699221283 +0900
@@ -0,0 +1,5 @@
+#!/bin/sh
+make -j3;
+make -j3 modules;
+make -j3 modules_install;
+make -j3 install;
Binary files c/net/core/.net-procfs.c.swp and d/net/core/.net-procfs.c.swp differ
diff -ruN c/net/ipv4/tcp.c d/net/ipv4/tcp.c
--- c/net/ipv4/tcp.c	2015-08-04 16:50:26.943554633 +0900
+++ d/net/ipv4/tcp.c	2015-09-18 23:58:27.947197013 +0900
@@ -1308,6 +1308,7 @@
 
 			from += copy;
 			copied += copy;
+
 			if ((seglen -= copy) == 0 && iovlen == 0)
 				goto out;
 
@@ -2525,6 +2526,28 @@
 		release_sock(sk);
 		return err;
 	}
+#ifdef CONFIG_MPTCP
+	case MPTCP_SCHEDULER: {
+		char name[MPTCP_SCHED_NAME_MAX];
+
+		if (!mptcp(tp))
+			return -EPERM;
+
+		if (optlen < 1)
+			return -EINVAL;
+
+		val = strncpy_from_user(name, optval,
+					min_t(long, MPTCP_SCHED_NAME_MAX-1, optlen));
+		if (val < 0)
+			return -EFAULT;
+		name[val] = 0;
+
+		lock_sock(sk);
+		err = mptcp_set_scheduler(sk, name);
+		release_sock(sk);
+		return err;
+	}
+#endif
 	default:
 		/* fallthru */
 		break;
@@ -3009,6 +3032,44 @@
 	case MPTCP_ENABLED:
 		val = sock_flag(sk, SOCK_MPTCP) ? 1 : 0;
 		break;
+
+	case MPTCP_SCHEDULER:
+		if (!mptcp(tp) || !(tp->mpcb)) 
+			return -EPERM;
+		if (get_user(len, optlen))
+			return -EFAULT;
+		len = min_t(unsigned int, len, MPTCP_SCHED_NAME_MAX);
+		if (put_user(len, optlen))
+			return -EFAULT;
+		if (copy_to_user(optval, tp->mpcb->sched_ops->name, len))
+			return -EFAULT;
+		return 0;
+#endif
+#ifdef CONFIG_NC_KT_MAGW /* 09.07 */
+	case MAGW_NETPROTO: {
+		u32	vals[2] = {0,};
+
+		if (get_user(len, optlen))
+			return -EFAULT;
+
+		if (mptcp(tp) && tp->mpcb) {
+			vals[0] = tp->netProto; /*1:WIFI,2:LTE*/
+			vals[1] = tp->mpcb->mptcp_loc_token; 
+			//pr_alert("[%s] MPTCP type[%d] tok[%x], tp[%p]sk[%p]mpcb[%p]", __func__, vals[0], vals[1], tp, sk, tp->mpcb);
+		}
+		else {
+			vals[0]= 0;/*TCP*/
+			vals[1]= 0xffffffff;/*TCP*/
+			//pr_alert("[%s] TCP type[%d] tok[%x], tp[%p]sk[%p]", __func__, vals[0], vals[1], tp, sk);
+		}
+
+		len = min_t(unsigned int, len, sizeof(vals));
+		if (put_user(len, optlen))
+			return -EFAULT;
+		if (copy_to_user(optval, vals, len))
+			return -EFAULT;
+		return 0;
+	}
 #endif
 	default:
 		return -ENOPROTOOPT;
diff -ruN c/net/ipv4/tcp_input.c d/net/ipv4/tcp_input.c
--- c/net/ipv4/tcp_input.c	2015-08-04 16:50:26.945554606 +0900
+++ d/net/ipv4/tcp_input.c	2015-08-12 23:00:54.226376126 +0900
@@ -4406,6 +4406,10 @@
 
 	tp->rx_opt.dsack = 0;
 
+#ifdef CONFIG_NC_KT_MAGW 
+	nc_inc_rx_stat(tp, skb->len);
+#endif
+
 	/*  Queue data for delivery to the user.
 	 *  Packets in sequence go to the receive queue.
 	 *  Out of sequence packets to the out_of_order_queue.
diff -ruN c/net/ipv4/tcp_timer.c d/net/ipv4/tcp_timer.c
--- c/net/ipv4/tcp_timer.c	2015-08-04 16:50:26.947554579 +0900
+++ d/net/ipv4/tcp_timer.c	2015-08-04 18:02:51.081402083 +0900
@@ -172,8 +172,12 @@
 		syn_set = true;
 		/* Stop retransmitting MP_CAPABLE options in SYN if timed out. */
 		if (tcp_sk(sk)->request_mptcp &&
-		    icsk->icsk_retransmits >= mptcp_sysctl_syn_retries())
+		    icsk->icsk_retransmits >= mptcp_sysctl_syn_retries()) {
+
 			tcp_sk(sk)->request_mptcp = 0;
+			
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLERETRANSFALLBACK);
+		}
 	} else {
 		if (retransmits_timed_out(sk, sysctl_tcp_retries1, 0, 0)) {
 			/* Black hole detection */
diff -ruN c/net/mptcp/Kconfig d/net/mptcp/Kconfig
--- c/net/mptcp/Kconfig	2015-08-04 16:50:26.988554021 +0900
+++ d/net/mptcp/Kconfig	2015-09-30 02:06:13.558019761 +0900
@@ -8,6 +8,12 @@
           This replaces the normal TCP stack with a Multipath TCP stack,
           able to use several paths at once.
 
+config NC_KT_MAGW
+    bool "KT MAGW features"
+    depends on MPTCP=y
+    ---help---
+    This enable Add-on for KT MAGW.
+
 menuconfig MPTCP_PM_ADVANCED
 	bool "MPTCP: advanced path-manager control"
 	depends on MPTCP=y
@@ -85,6 +91,27 @@
 	  This is a very simple round-robin scheduler. Probably has bad performance
 	  but might be interesting for researchers.
 
+config MPTCP_REDUNDANT
+	tristate "MPTCP Redundant"
+	depends on (MPTCP=y)
+	---help---
+	  This is a very simple redundant scheduler, which floods each MPTCP segment
+	  through all the available TCP subflows. It has a bad throughput but it might
+	  be interesting for researchers focused on applications with low throughput but
+	  high latency and jitter demands. 
+
+config MPTCP_KTSCHED01
+	tristate "MPTCP KT Scheduler 01"
+	depends on (MPTCP=y)
+	---help---
+	  This is only for KT Magw test scheduler number 01.
+
+config MPTCP_KTSCHED02
+	tristate "MPTCP KT Scheduler 02"
+	depends on (MPTCP=y)
+	---help---
+	  This is only for KT Magw test scheduler number 02.
+
 choice
 	prompt "Default MPTCP Scheduler"
 	default DEFAULT
@@ -103,6 +130,22 @@
 		  This is the round-rob scheduler, sending in a round-robin
 		  fashion..
 
+	config DEFAULT_REDUNDANT
+		bool "Redundant" if MPTCP_REDUNDANT=y
+		---help---
+		  This is the redundant scheduler, sending each MPTCP segment
+		  through all the available TCP subflows...
+
+	config DEFAULT_KTSCHED01
+		bool "KT01" if MPTCP_KTSCHED01=y
+		---help---
+		  This is the KT MAGW scheduler.
+
+	config DEFAULT_KTSCHED02
+		bool "KT02" if MPTCP_KTSCHED02=y
+		---help---
+		  This is the KT MAGW scheduler. algorithm #2.
+
 endchoice
 endif
 
@@ -111,5 +154,8 @@
 	depends on (MPTCP=y)
 	default "default" if DEFAULT_SCHEDULER
 	default "roundrobin" if DEFAULT_ROUNDROBIN
+	default "redundant" if DEFAULT_REDUNDANT
+	default "kt01" if DEFAULT_KTSCHED01
+	default "kt02" if DEFAULT_KTSCHED02
 	default "default"
 
diff -ruN c/net/mptcp/Makefile d/net/mptcp/Makefile
--- c/net/mptcp/Makefile	2015-08-04 16:50:26.988554021 +0900
+++ d/net/mptcp/Makefile	2015-09-30 02:06:29.360780374 +0900
@@ -15,6 +15,11 @@
 obj-$(CONFIG_MPTCP_NDIFFPORTS) += mptcp_ndiffports.o
 obj-$(CONFIG_MPTCP_BINDER) += mptcp_binder.o
 obj-$(CONFIG_MPTCP_ROUNDROBIN) += mptcp_rr.o
+obj-$(CONFIG_MPTCP_REDUNDANT) += mptcp_redundant.o
+obj-$(CONFIG_MPTCP_KTSCHED01) += mptcp_kt01.o
+obj-$(CONFIG_MPTCP_KTSCHED02) += mptcp_kt02.o
+obj-$(CONFIG_NC_KT_MAGW) += mptcp_sm.o
+obj-$(CONFIG_NC_KT_MAGW) += magw_util.o
 
 mptcp-$(subst m,y,$(CONFIG_IPV6)) += mptcp_ipv6.o
 
diff -ruN c/net/mptcp/magw_util.c d/net/mptcp/magw_util.c
--- c/net/mptcp/magw_util.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/magw_util.c	2015-09-16 22:55:31.224023048 +0900
@@ -0,0 +1,652 @@
+#include <net/inet_common.h>
+#include <net/inet6_hashtables.h>
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#include <net/mptcp.h>
+#include <net/mptcp_v4.h>
+#if IS_ENABLED(CONFIG_IPV6)
+#include <net/ip6_route.h>
+#include <net/mptcp_v6.h>
+#endif
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/tcp_states.h>
+#include <net/transp_v6.h>
+#include <net/xfrm.h>
+
+#include <linux/cryptohash.h>
+#include <linux/kconfig.h>
+#include <linux/module.h>
+#include <linux/netpoll.h>
+#include <linux/list.h>
+#include <linux/jhash.h>
+#include <linux/tcp.h>
+#include <linux/net.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/random.h>
+#include <linux/inetdevice.h>
+#include <linux/workqueue.h>
+#include <linux/atomic.h>
+#include <linux/sysctl.h>
+
+#include <linux/module.h>
+#include <net/mptcp.h>
+
+/* ------------------------------------------------------------------------*/
+/*  Global Variables                                                       */
+/* ------------------------------------------------------------------------*/
+
+static lfRTB  lfRtb = {
+    .idx = 0,
+    .tb[0] = {.cnt=0, .r[0] = {0,0,0},},
+    .tb[1] = {.cnt=0, .r[0] = {0,0,0},},
+};
+
+static lfGwTB lfGwtb = {
+    .idx = 0,
+    .tb[0]= {.addr4={0,}, .addr6={{{{0,}},}},},
+    .tb[1]= {.addr4={0,}, .addr6={{{{0,}},}},},
+};
+
+/* ------------------------------------------------------------------------*/
+/* Updates the MAGW-Statistics in tcp_sock->mptcp 
+ */
+void nc_inc_rx_stat(struct tcp_sock *tp, u32 len)
+{
+	if(!mptcp(tp)) goto END;
+
+	NC_INC_RX(tp->mptcp->nc_stat, len);
+END:
+	return;
+} /* end of function */
+
+/* ------------------------------------------------------------------------*/
+/* Updates the MAGW-Statistics in tcp_sock->mptcp 
+ */
+void nc_inc_tx_stat(struct tcp_sock *tp, u32 len)
+{
+	if(!mptcp(tp)) goto END;
+
+	NC_INC_TX(tp->mptcp->nc_stat, len);
+END:
+	return;
+} /* end of function */
+
+
+/* ------------------------------------------------------------------------*/
+void gwSetIPv4(u8 idx, const char *addr, int len) {
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+	if(idx>=2) return;
+	in4_pton(addr, -1, (u8*)&gtb->addr4[idx], -1, NULL);
+	return;
+} /* end of function */
+EXPORT_SYMBOL(gwSetIPv4);
+
+/* ------------------------------------------------------------------------*/
+void gwGetIPv4(u8 idx, char *addr, int len) {
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+	if(idx>=2) return;
+	memset(addr, 0x00, len);	
+	snprintf(addr, len, "%pI4", &gtb->addr4[idx]);
+	return;
+} /* end of function */
+EXPORT_SYMBOL(gwGetIPv4);
+
+/* ------------------------------------------------------------------------*/
+void gwSetIPv6(u8 idx, const char *addr, int len) {
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+	if(idx>=2) return;
+
+	in6_pton(addr, -1, gtb->addr6[idx].s6_addr, -1, NULL);
+	return;
+} /* end of function */
+EXPORT_SYMBOL(gwSetIPv6);
+
+/* ------------------------------------------------------------------------*/
+void gwGetIPv6(u8 idx, char *addr, int len) {
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+	if(idx>=2) return;
+	memset(addr, 0x00, len);	
+	snprintf(addr, len, "%pI6", gtb->addr6[idx].s6_addr);
+	return;
+} /* end of function */
+EXPORT_SYMBOL(gwGetIPv6);
+
+
+#define MAX(_a, _b) ((_a > _b)?_a:_b)
+/* ------------------------------------------------------------------------*/
+void rSetRow(int idx, u32 uS, u32 uE) {
+
+RTB *rtb = &lfRtb.tb[lfRtb.idx];
+
+    if(idx>=MAX_RTB_ROWS)   return;
+
+    rtb->r[idx].s = ntohl(uS);
+    rtb->r[idx].e = ntohl(uE);
+    rtb->r[idx].used = 1;
+	pr_crit("[%s] %3d [%pI4(%u)]~[%pI4(%u)]\n", __func__, idx, 
+			&uS, rtb->r[idx].s, &uE, rtb->r[idx].e);
+
+	rtb->cnt = MAX(rtb->cnt, idx);
+    return;
+} /* end of function */
+EXPORT_SYMBOL(rSetRow);
+
+
+/* ------------------------------------------------------------------------*/
+void lfRTBUpdate(void *param) {
+RTB *sR = param;
+int  i=0;
+
+int  nIdx = (lfRtb.idx+1)%2;
+RTB *dR = &lfRtb.tb[nIdx];
+
+	if(sR == NULL) return;
+	if(sR->cnt<0 || sR->cnt>=MAX_RTB_ROWS) return;
+
+	for(i=0; i<MAX_RTB_ROWS; i++)
+	{
+		if(sR->r[i].used)
+		{
+			memcpy(dR->r+i, sR->r+i, sizeof(R4L));
+
+			magw_st_debug("[%s] %3d [%pI4]~[%pI4]\n", __func__, i, 
+					   &(dR->r[i].s), &(dR->r[i].e));
+		}
+		else
+			memset(dR->r+i, 0x00, sizeof(R4L));
+	} /* end of for */
+
+	dR->cnt = sR->cnt;
+
+	magw_st_debug("[%s] New index [%d]->[%d]\n", __func__, lfRtb.idx, nIdx);
+
+	/* Update Index */
+	lfRtb.idx = nIdx;
+
+    return;
+} /* end of function */
+EXPORT_SYMBOL(lfRTBUpdate);
+
+/* ------------------------------------------------------------------------*/
+int isIPv4MagwSvc(uint32_t host, u8 *idx) {
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+u8 i = 0;
+	for(i=0; i<2; i++) {
+		if(gtb->addr4[i] == host) {
+			*idx = i;
+			return 1;
+		}
+	} /* end of for */
+	*idx=i;
+	return 0;
+} /* end of if */
+EXPORT_SYMBOL(isIPv4MagwSvc);
+
+/* ------------------------------------------------------------------------*/
+int isIPv4LTE(uint32_t host) {
+int  i = 0;
+RTB *rtb = &lfRtb.tb[lfRtb.idx%2];
+R4L *r = NULL;
+uint32_t h = ntohl(host);
+    for(i=0; i<rtb->cnt; i++) {
+        r = &rtb->r[i];
+		if(r->used ==0) continue;
+        if(h>=r->s && h<=r->e)
+            return 1;
+    } /* end of for */
+
+    return 0;
+} /* end of if */
+EXPORT_SYMBOL(isIPv4LTE);
+
+
+#define IDX_FLAG		0
+#define IDX_IPV4_V6		1
+#define IDX_NIC_INTF	2
+#define IDX_LTE_WIFI	3
+/* ------------------------------------------------------------------------*/
+inline 
+void mgCheckStat(struct sock *sk, struct sk_buff *skb, u8 *vals, char *name)
+{
+const struct iphdr  *iph = ip_hdr(skb);
+const struct tcphdr *th  = tcp_hdr(skb);
+struct inet_sock    *ik  = inet_sk(sk);
+
+	if(tcp_hdr(skb)->dest != mptcp_gw_port) 
+		goto DONT_INC_STAT;
+	
+	if(!isIPv4MagwSvc(ip_hdr(skb)->daddr, &vals[IDX_NIC_INTF/*2*/])) 
+		goto DONT_INC_STAT;
+
+	/* Now Valid Service*/
+	if (skb->protocol == htons(ETH_P_IP))
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV4; 
+	else
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV6; 
+
+	 if(isIPv4LTE(iph->saddr))
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_LTE; 
+	 else
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_WIFI; 
+
+	vals[IDX_FLAG/*0*/]= 1; 
+
+	magw_st_debug("[%s]->[%s]:HDR(S[%pI4:%d]D[%pI4:%d]),IK(s[%pI4:%d]d[%pI4:%d])SK(d[%pI4:%d][%p])[%d,%d,%d,%d]", 
+		name, __func__, 
+		&iph->saddr, ntohs(th->source), 
+		&iph->daddr, ntohs(th->dest),
+		&ik->inet_saddr, ntohs(ik->inet_sport),
+		&ik->inet_daddr, ntohs(ik->inet_dport),
+		&sk->sk_daddr, ntohs(sk->sk_dport), sk, vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+	
+DONT_INC_STAT:
+	vals[IDX_FLAG/*0*/]= 0; 
+
+	magw_st_debug("[%s]->[%s]:DO_NOT_INC:HDR(S[%pI4:%d]D[%pI4:%d]),IK(s[%pI4:%d]d[%pI4:%d])SK(d[%pI4:%d][%p])[%d,%d,%d,%d]", 
+		name, __func__, 
+		&iph->saddr, ntohs(th->source), 
+		&iph->daddr, ntohs(th->dest),
+		&ik->inet_saddr, ntohs(ik->inet_sport),
+		&ik->inet_daddr, ntohs(ik->inet_dport),
+		&sk->sk_daddr, ntohs(sk->sk_dport), sk, vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+} /* end of function */
+EXPORT_SYMBOL(mgCheckStat);
+
+
+/* ------------------------------------------------------------------------*/
+inline 
+void mgCheckStat2(struct sock *sk, struct sock *child, u8 *vals, char *name)
+{
+	if(inet_sk(child)->inet_sport != mptcp_gw_port) 
+		goto DONT_INC_STAT;
+	
+	if(!isIPv4MagwSvc(inet_sk(child)->inet_saddr, &vals[IDX_NIC_INTF/*2*/])) 
+		goto DONT_INC_STAT;
+
+	/* Now Valid Service*/
+	if (child->sk_family == AF_INET)
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV4; 
+	else
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV6; 
+
+	 if(isIPv4LTE(inet_sk(child)->inet_daddr))
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_LTE; 
+	 else
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_WIFI; 
+
+	vals[IDX_FLAG/*0*/]= 1; 
+
+	magw_st_debug("[%s]->[%s]:CHD(S[%pI4:%d]D[%pI4:%d]),SK(s[%pI4:%d]d[%pI4:%d])SK(%p)CHD(%p)[%d,%d,%d,%d]", 
+		name, __func__, 
+		&inet_sk(child)->inet_saddr, ntohs(inet_sk(child)->inet_sport), 
+		&inet_sk(child)->inet_daddr, ntohs(inet_sk(child)->inet_dport), 
+		&inet_sk(sk)->inet_saddr, ntohs(inet_sk(sk)->inet_sport), 
+		&inet_sk(sk)->inet_daddr, ntohs(inet_sk(sk)->inet_dport), 
+		sk, child, vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+	
+DONT_INC_STAT:
+	vals[IDX_FLAG/*0*/]= 0; 
+
+	magw_st_debug("[%s]->[%s]:DO_NOT_INC:CHD(S[%pI4:%d]D[%pI4:%d]),SK(s[%pI4:%d]d[%pI4:%d])SK(%p)CHD(%p)[%d,%d,%d,%d]", 
+		name, __func__, 
+		&inet_sk(child)->inet_saddr, ntohs(inet_sk(child)->inet_sport), 
+		&inet_sk(child)->inet_daddr, ntohs(inet_sk(child)->inet_dport), 
+		&inet_sk(sk)->inet_saddr, ntohs(inet_sk(sk)->inet_sport), 
+		&inet_sk(sk)->inet_daddr, ntohs(inet_sk(sk)->inet_dport), 
+		sk, child, vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+} /* end of function */
+EXPORT_SYMBOL(mgCheckStat2);
+
+/* ------------------------------------------------------------------------*/
+inline 
+void mgCheckStat3(struct sk_buff *skb, u8 *vals, char *name)
+{
+const struct iphdr  *iph = ip_hdr(skb);
+const struct tcphdr *th  = tcp_hdr(skb);
+
+	if(tcp_hdr(skb)->dest != mptcp_gw_port) 
+		goto DONT_INC_STAT;
+	
+	if(!isIPv4MagwSvc(ip_hdr(skb)->daddr, &vals[IDX_NIC_INTF/*2*/])) 
+		goto DONT_INC_STAT;
+
+	/* Now Valid Service*/
+	if (skb->protocol == htons(ETH_P_IP))
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV4; 
+	else
+		vals[IDX_IPV4_V6/*1*/]= MAGW_FM_IPV6; 
+
+	 if(isIPv4LTE(iph->saddr))
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_LTE; 
+	 else
+		vals[IDX_LTE_WIFI/*3*/]= MAGW_SVC_WIFI; 
+
+	vals[IDX_FLAG/*0*/]= 1; 
+
+	magw_st_debug("[%s]->[%s]:HDR(S[%pI4:%d]D[%pI4:%d])[%d,%d,%d,%d]", 
+		name, __func__, 
+		&iph->saddr, ntohs(th->source), 
+		&iph->daddr, ntohs(th->dest),
+		 vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+	
+DONT_INC_STAT:
+	vals[IDX_FLAG/*0*/]= 0; 
+
+	magw_st_debug("[%s]->[%s]:DO_NOT_INC:HDR(S[%pI4:%d]D[%pI4:%d]),[%d,%d,%d,%d]", 
+		name, __func__, 
+		&iph->saddr, ntohs(th->source), 
+		&iph->daddr, ntohs(th->dest),
+		vals[0],vals[1],vals[2], vals[3]);
+
+	return;
+} /* end of function */
+EXPORT_SYMBOL(mgCheckStat3);
+
+
+
+/* ------------------------------------------------------------------------*/
+/* MAGW MPTCP Statistics */
+/* ------------------------------------------------------------------------*/
+
+static const struct snmp_mib magw_snmp_list[] = {
+    SNMP_MIB_ITEM("MP_CAPABLE_SYN",     MAGW_MIB_MCSYN_RX),/**/
+    SNMP_MIB_ITEM("MP_CAPABLE_SYN_ACK", MAGW_MIB_MCSACK_TX),/**/
+    SNMP_MIB_ITEM("MP_CAPABLE_ACK", 	MAGW_MIB_MCACK_RX),/**/
+    SNMP_MIB_ITEM("MP_JOIN_SYN",        MAGW_MIB_MJSYN_RX),/**/
+    SNMP_MIB_ITEM("MP_JOIN_SYN_ACK",    MAGW_MIB_MJSACK_TX),/**/
+    SNMP_MIB_ITEM("MP_JOIN_ACK", 	    MAGW_MIB_MJACK_RX),/**/
+    SNMP_MIB_ITEM("MP_FAIL_SEND", 	    MAGW_MIB_FAIL_TX),
+    SNMP_MIB_ITEM("MP_FAIL_RECV", 	    MAGW_MIB_FAIL_RX),/**/
+    SNMP_MIB_ITEM("MP_FASTCLOSE_SEND",  MAGW_MIB_FCLOSE_TX),
+    SNMP_MIB_ITEM("MP_FASTCLOSE_RECV",  MAGW_MIB_FCLOSE_RX),/**/
+    SNMP_MIB_ITEM("DATA_FIN_SEND",      MAGW_MIB_DTFIN_TX),
+    SNMP_MIB_ITEM("DATA_FIN_RECV",      MAGW_MIB_DTFIN_RX),/**/
+    SNMP_MIB_ITEM("REMOVE_ADDR_SEND",   MAGW_MIB_RMADDR_TX),/**/
+    SNMP_MIB_ITEM("REMOVE_ADDR_RECV",   MAGW_MIB_RMADDR_RX),/**/
+    SNMP_MIB_ITEM("ADD_ADDR_SEND",      MAGW_MIB_ADADDR_TX),/**/
+    SNMP_MIB_ITEM("ADD_ADDR_RECV",      MAGW_MIB_ADADDR_RX),/**/
+ /* MPTCP Fail */
+    SNMP_MIB_ITEM("TRY",                 MAGW_MIB_MP_TRY),/**/
+    SNMP_MIB_ITEM("SUCCESS",             MAGW_MIB_MP_SUCCESS),/**/
+    SNMP_MIB_ITEM("FAILURE",             MAGW_MIB_MP_FAIL),/**/
+    SNMP_MIB_ITEM("CAPABLE_SYN_ERR",     MAGW_MIB_MP_CYE),
+    SNMP_MIB_ITEM("CAPABLE_TIMEOUT_ERR", MAGW_MIB_MP_CTE),
+    SNMP_MIB_ITEM("CAPABLE_ACK_ERR",     MAGW_MIB_MP_CAE),
+    SNMP_MIB_ITEM("CAPABLE_CHECKSUM_ERR",MAGW_MIB_MP_CCE),
+    SNMP_MIB_ITEM("JOIN_SYN_ERR",        MAGW_MIB_MP_JSE),
+    SNMP_MIB_ITEM("JOIN_TIMEOUT_ERR",    MAGW_MIB_MP_JTE),
+    SNMP_MIB_ITEM("JOIN_ACK_ERR",        MAGW_MIB_MP_JAE),
+    SNMP_MIB_ITEM("JOIN_CHECKSUM_ERR",   MAGW_MIB_MP_JCE),
+    SNMP_MIB_SENTINEL
+};
+
+/* ------------------------------------------------------------------------*
+ *  Output /proc/net/mptcp_net/magw
+ */
+static int magw_snmp_seq_show(struct seq_file *seq, void *v)
+{
+int i=0;
+struct net *net = seq->private;
+GwTB *gtb = &lfGwtb.tb[lfGwtb.idx];
+
+#if 0
+	if(gtb->addr4[0] == INADDR_ANY)
+		goto NET_INTF_V4_2;
+#endif
+	seq_printf(seq, "IPv4: %pI4", &gtb->addr4[0]);
+	seq_puts(seq, "\nLTE :");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i1v4L_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_puts(seq, "\nWIFI:");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++)
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i1v4W_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_putc(seq, '\n');
+
+#if 0
+NET_INTF_V4_2:
+	if(gtb->addr4[1] == INADDR_ANY)
+		goto NET_INTF_V6_1;
+#endif
+	seq_printf(seq, "IPv4: %pI4", &gtb->addr4[1]);
+	seq_puts(seq, "\nLTE :");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i2v4L_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_puts(seq, "\nWIFI:");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+	
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i2v4W_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_putc(seq, '\n');
+
+#if 0
+NET_INTF_V6_1:
+	if(!memcmp(&gtb->addr6[0], &in6addr_any, sizeof(struct in6_addr)))
+		goto NET_INTF_V6_2;
+#endif
+
+	seq_printf(seq, "IPv6: %pI6", &gtb->addr6[0]);
+	seq_puts(seq, "\nLTE :");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+	
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i1v6L_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_puts(seq, "\nWIFI:");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++)
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+	
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i1v6W_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_putc(seq, '\n');
+
+#if 0
+NET_INTF_V6_2:
+	if(!memcmp(&gtb->addr6[1], &in6addr_any, sizeof(struct in6_addr)))
+		goto DONE;
+#endif
+
+	seq_printf(seq, "IPv6: %pI6", &gtb->addr6[1]);
+	seq_puts(seq, "\nLTE :");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+	
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i2v6L_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_puts(seq, "\nWIFI:");
+	for (i = 0; magw_snmp_list[i].name != NULL; i++) 
+	{
+		if(magw_snmp_list[i].entry >=MAGW_MIB_RMADDR_TX && 
+				magw_snmp_list[i].entry <=MAGW_MIB_ADADDR_RX) continue;
+		if(magw_snmp_list[i].entry == MAGW_MIB_MP_FAIL) continue;
+	
+		seq_printf(seq, " %lu", 
+			   snmp_fold_field((void __percpu **) net->mptcp.mg_i2v6W_stat,
+				      magw_snmp_list[i].entry));
+	}
+	seq_putc(seq, '\n');
+#if 0
+DONE:
+	seq_putc(seq, '\n');
+#endif
+	return 0;
+} /* end of function */
+
+
+/* ------------------------------------------------------------------------*/
+static int magw_snmp_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open_net(inode, file, magw_snmp_seq_show);
+}
+
+static const struct file_operations magw_snmp_seq_fops = {
+    .owner = THIS_MODULE,
+    .open  = magw_snmp_seq_open,
+    .read  = seq_read,
+    .llseek = seq_lseek,
+    .release = single_release_net,
+};
+
+
+/* ------------------------------------------------------------------------*/
+int magw_mib_init_net(struct net *net)
+{
+	/* ------------ */
+	/* For IPv4     */
+	/* ------------ */
+	/* interface #1 - LTE */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i1v4L_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_0;
+
+	/* interface #1 - WIFI */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i1v4W_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_1;
+
+	/* interface #2 - LTE */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i2v4L_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_2;
+
+	/* interface #2 - WIFI */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i2v4W_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_3;
+
+
+	/* ------------ */
+	/* For IPv6 */
+	/* ------------ */
+	/* interface #1 - LTE */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i1v6L_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_4;
+
+	/* interface #1 - WIFI */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i1v6W_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_5;
+
+	/* interface #2 - LTE */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i2v6L_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_6;
+
+	/* interface #2 - WIFI */
+	if (snmp_mib_init((void __percpu **)net->mptcp.mg_i2v6W_stat, 
+			sizeof(struct magw_mib), __alignof__(struct magw_mib)) < 0)
+		goto ERROR_7;
+
+	return 0;
+
+ERROR_7:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v6L_stat);
+ERROR_6:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v6W_stat);
+ERROR_5:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v6L_stat);
+ERROR_4:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v4W_stat);
+ERROR_3:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v4L_stat);
+ERROR_2:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v4W_stat);
+ERROR_1:
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v4L_stat);
+ERROR_0:
+
+	return -ENOMEM;
+} /* end of function */
+
+/* ------------------------------------------------------------------------*/
+void magw_mib_exit_net(struct net *net)
+{
+	/* For IPv4 */
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v4L_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v4W_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v4L_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v4W_stat);
+	/* For IPv6 */
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v6L_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i1v6W_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v6L_stat);
+	snmp_mib_free((void __percpu **)net->mptcp.mg_i2v6W_stat);
+
+} /* end of function */
+
+
+/* ------------------------------------------------------------------------*/
+int magw_proc_init_net(struct net *net)
+{
+    if(!proc_create("magw", S_IRUGO, net->mptcp.proc_net_mptcp, 
+									&magw_snmp_seq_fops))
+		return -ENOMEM;
+	return 1;
+}
+
+/* ------------------------------------------------------------------------*/
+void magw_proc_exit_net(struct net *net)
+{
+	remove_proc_entry("magw", net->mptcp.proc_net_mptcp);
+} /* end of function */
+
+/* ------------------------------------------------------------------------*/
+/*  END OF FILE                                                            */
+/* ------------------------------------------------------------------------*/
diff -ruN c/net/mptcp/mptcp_ctrl.c d/net/mptcp/mptcp_ctrl.c
--- c/net/mptcp/mptcp_ctrl.c	2015-08-04 16:50:26.989554008 +0900
+++ d/net/mptcp/mptcp_ctrl.c	2015-09-18 23:44:04.999658177 +0900
@@ -66,6 +66,18 @@
 int sysctl_mptcp_checksum __read_mostly = 1;
 int sysctl_mptcp_debug __read_mostly;
 EXPORT_SYMBOL(sysctl_mptcp_debug);
+#ifdef CONFIG_NC_KT_MAGW
+/*in sysctl*/
+int sysctl_magw_st_debug __read_mostly = 0;
+int sysctl_mptcp_sm_debug __read_mostly = 1;
+int sysctl_mptcp_gw_port  __read_mostly = 7683;//0907
+EXPORT_SYMBOL(sysctl_magw_st_debug);
+EXPORT_SYMBOL(sysctl_mptcp_sm_debug);
+EXPORT_SYMBOL(sysctl_mptcp_gw_port);//0907
+/*not in sysctl - just global */
+u16 mptcp_gw_port  __read_mostly = htons(7683);//0907
+EXPORT_SYMBOL(mptcp_gw_port);//0907
+#endif
 int sysctl_mptcp_syn_retries __read_mostly = 3;
 
 bool mptcp_init_failed __read_mostly;
@@ -73,6 +85,121 @@
 struct static_key mptcp_static_key = STATIC_KEY_INIT_FALSE;
 EXPORT_SYMBOL(mptcp_static_key);
 
+
+#ifdef CONFIG_NC_KT_MAGW
+static int proc_magw_svr_ipv4_1(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+char val[MPTCP_SM_ADDR_MAX] = {0,};
+ctl_table tbl = {
+	.data = val,
+	.maxlen = MPTCP_SM_NAME_MAX,
+};
+int ret;
+
+	gwGetIPv4(0, val, MPTCP_SM_ADDR_MAX);
+
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		gwSetIPv4(0, (const char*)val, 0);
+	return ret;
+}
+
+static int proc_magw_svr_ipv4_2(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+char val[MPTCP_SM_ADDR_MAX] = {0,};
+ctl_table tbl = {
+	.data = val,
+	.maxlen = MPTCP_SM_NAME_MAX,
+};
+int ret;
+
+	gwGetIPv4(1, val, MPTCP_SM_ADDR_MAX);
+
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		gwSetIPv4(1, (const char*)val, 0);
+	return ret;
+}
+
+static int proc_magw_svr_ipv6_1(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+char val[MPTCP_SM_ADDR_MAX] = {0,};
+ctl_table tbl = {
+	.data = val,
+	.maxlen = MPTCP_SM_NAME_MAX,
+};
+int ret;
+
+	gwGetIPv6(0/*intf#1*/, val, MPTCP_SM_ADDR_MAX);
+
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		gwSetIPv6(0/*intf#1*/, (const char*)val, 0);
+	return ret;
+}
+
+static int proc_magw_svr_ipv6_2(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+char val[MPTCP_SM_ADDR_MAX] = {0,};
+ctl_table tbl = {
+	.data = val,
+	.maxlen = MPTCP_SM_NAME_MAX,
+};
+int ret;
+
+	gwGetIPv6(1/*intf#2*/, val, MPTCP_SM_ADDR_MAX);
+
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		gwSetIPv6(1/*intf#2*/, (const char*)val, 0);
+	return ret;
+}
+
+
+
+static int proc_mptcp_session_monitor(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+	char val[MPTCP_SM_NAME_MAX];
+	ctl_table tbl = {
+		.data = val,
+		.maxlen = MPTCP_SM_NAME_MAX,
+	};
+	int ret;
+
+	mptcp_get_default_session_monitor(val);
+
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		ret = mptcp_set_default_session_monitor(val);
+	return ret;
+}
+/* 0907 */
+static int proc_mptcp_gw_port(ctl_table *ctl, int write,
+				   void __user *buffer, size_t *lenp,
+				   loff_t *ppos)
+{
+	int ret;
+	int oldPort = sysctl_mptcp_gw_port;
+
+	ret = proc_dointvec(ctl, write, buffer, lenp, ppos);
+	if(sysctl_mptcp_gw_port <= 0 || sysctl_mptcp_gw_port>65535)
+		sysctl_mptcp_gw_port = oldPort;
+
+	mptcp_gw_port = htons((u16)sysctl_mptcp_gw_port);
+	return ret;
+}
+#endif
+
 static int proc_mptcp_path_manager(ctl_table *ctl, int write,
 				   void __user *buffer, size_t *lenp,
 				   loff_t *ppos)
@@ -92,6 +219,45 @@
 	return ret;
 }
 
+#if 1 //0917
+static int proc_mptcp_available_scheduler(struct ctl_table *ctl,
+                         int write,
+                         void __user *buffer, size_t *lenp,
+                         loff_t *ppos)
+{
+    struct ctl_table tbl = { .maxlen = MPTCP_SCHED_BUF_MAX, };
+    int ret;
+
+    tbl.data = kmalloc(tbl.maxlen, GFP_USER);
+    if (!tbl.data)
+        return -ENOMEM;
+    mptcp_get_available_scheduler(tbl.data, MPTCP_SCHED_BUF_MAX);
+    ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+    kfree(tbl.data);
+    return ret;
+}
+
+static int proc_mptcp_allowed_scheduler(struct ctl_table *ctl,
+					   int write,
+					   void __user *buffer, size_t *lenp,
+					   loff_t *ppos)
+{
+	struct ctl_table tbl = { .maxlen = MPTCP_SCHED_BUF_MAX };
+	int ret;
+
+	tbl.data = kmalloc(tbl.maxlen, GFP_USER);
+	if (!tbl.data)
+		return -ENOMEM;
+
+	mptcp_get_allowed_scheduler(tbl.data, tbl.maxlen);
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0)
+		ret = mptcp_set_allowed_scheduler(tbl.data);
+	kfree(tbl.data);
+	return ret;
+}
+#endif
+
 static int proc_mptcp_scheduler(ctl_table *ctl, int write,
 				void __user *buffer, size_t *lenp,
 				loff_t *ppos)
@@ -133,6 +299,61 @@
 		.mode = 0644,
 		.proc_handler = &proc_dointvec
 	},
+#ifdef CONFIG_NC_KT_MAGW
+	{//0907
+		.procname = "mptcp_gw_port",
+		.data = &sysctl_mptcp_gw_port,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		/* .proc_handler = &proc_dointvec */
+		.proc_handler = proc_mptcp_gw_port,
+	},
+	{
+		.procname = "mptcp_sm_debug",
+		.data = &sysctl_mptcp_sm_debug,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname = "magw_st_debug",
+		.data = &sysctl_magw_st_debug,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname	= "mptcp_session_monitor",
+		.mode		= 0644,
+		.maxlen		= MPTCP_SM_NAME_MAX,
+		.proc_handler = proc_mptcp_session_monitor,
+	},
+
+	{
+		.procname	= "magw_svc_ipv4_1",
+		.mode		= 0644,
+		.maxlen		= MPTCP_SM_ADDR_MAX,
+		.proc_handler = proc_magw_svr_ipv4_1,
+	},
+	{
+		.procname	= "magw_svc_ipv4_2",
+		.mode		= 0644,
+		.maxlen		= MPTCP_SM_ADDR_MAX,
+		.proc_handler = proc_magw_svr_ipv4_2,
+	},
+	{
+		.procname	= "magw_svc_ipv6_1",
+		.mode		= 0644,
+		.maxlen		= MPTCP_SM_ADDR_MAX,
+		.proc_handler = proc_magw_svr_ipv6_1,
+	},
+	{
+		.procname	= "magw_svc_ipv6_2",
+		.mode		= 0644,
+		.maxlen		= MPTCP_SM_ADDR_MAX,
+		.proc_handler = proc_magw_svr_ipv6_2,
+	},
+#endif
 	{
 		.procname = "mptcp_syn_retries",
 		.data = &sysctl_mptcp_syn_retries,
@@ -146,6 +367,21 @@
 		.maxlen		= MPTCP_PM_NAME_MAX,
 		.proc_handler	= proc_mptcp_path_manager,
 	},
+#if 1 //0917
+    {
+        .procname   = "mptcp_available_scheduler",
+        //.maxlen     = TCP_CA_BUF_MAX,
+        .maxlen     = MPTCP_SCHED_BUF_MAX,
+        .mode       = 0444,
+        .proc_handler   = proc_mptcp_available_scheduler,
+    },
+    {
+        .procname   = "mptcp_allowed_scheduler",
+        .maxlen     = MPTCP_SCHED_BUF_MAX,
+        .mode       = 0644,
+        .proc_handler   = proc_mptcp_allowed_scheduler,
+    },
+#endif
 	{
 		.procname	= "mptcp_scheduler",
 		.mode		= 0644,
@@ -445,6 +681,8 @@
 	__mptcp_hash_insert(tp, tp->mptcp_loc_token);
 	spin_unlock(&mptcp_tk_hashlock);
 	rcu_read_unlock_bh();
+
+	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLEACTIVE);
 }
 
 /**
@@ -598,6 +836,9 @@
 	if (atomic_dec_and_test(&mpcb->mpcb_refcnt)) {
 		mptcp_cleanup_path_manager(mpcb);
 		mptcp_cleanup_scheduler(mpcb);
+#ifdef CONFIG_NC_KT_MAGW
+		mptcp_cleanup_session_monitor(mpcb);
+#endif
 		kmem_cache_free(mptcp_cb_cache, mpcb);
 	}
 }
@@ -632,6 +873,10 @@
 		}
 		spin_unlock_bh(&mpcb->tw_lock);
 
+#ifdef CONFIG_NC_KT_MAGW
+		if(mpcb->sm_ops->on_destory_session)
+			mpcb->sm_ops->on_destory_session(mpcb, sk);
+#endif
 		mptcp_mpcb_put(mpcb);
 
 		mptcp_debug("%s destroying meta-sk\n", __func__);
@@ -1308,6 +1553,9 @@
 
 	mptcp_init_path_manager(mpcb);
 	mptcp_init_scheduler(mpcb);
+#ifdef CONFIG_NC_KT_MAGW
+	mptcp_init_session_monitor(mpcb);
+#endif
 
 	setup_timer(&mpcb->synack_timer, mptcp_synack_timer_handler,
 		    (unsigned long)meta_sk);
@@ -1462,6 +1710,11 @@
 		    __func__, mpcb->mptcp_loc_token, tp->mptcp->path_index,
 		    sk->sk_state, is_meta_sk(sk));
 
+#ifdef CONFIG_NC_KT_MAGW
+	if(mpcb->sm_ops->on_del_session)
+		mpcb->sm_ops->on_del_session(mpcb, sk);
+#endif
+
 	if (tp_prev == tp) {
 		mpcb->connection_list = tp->mptcp->next;
 	} else {
@@ -1491,6 +1744,7 @@
 	rcu_assign_pointer(inet_sk(sk)->inet_opt, NULL);
 }
 
+
 /* Updates the MPTCP-session based on path-manager information (e.g., addresses,
  * low-prio flows,...).
  */
@@ -2040,6 +2294,9 @@
 	struct sock *meta_sk = child;
 	struct mptcp_cb *mpcb;
 	struct mptcp_request_sock *mtreq;
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	u8 Fs[4] = {0,};
+#endif
 
 	/* Never contained an MP_CAPABLE */
 	if (!inet_rsk(req)->mptcp_rqsk)
@@ -2051,10 +2308,19 @@
 		 * But, the socket has been added to the reqsk_tk_htb, so we
 		 * must still remove it.
 		 */
+#ifdef CONFIG_NC_KT_MAGW //09.12
+		mgCheckStat2(sk, child, Fs, "mptcp_check_req_master");
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_CYE);
+#endif
+
+		MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_MPCAPABLEPASSIVEFALLBACK);
 		mptcp_reqsk_remove_tk(req);
 		return 1;
 	}
 
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_MPCAPABLEPASSIVEACK);
+
 	/* Just set this values to pass them to mptcp_alloc_mpcb */
 	mtreq = mptcp_rsk(req);
 	child_tp->mptcp_loc_key = mtreq->mptcp_loc_key;
@@ -2062,7 +2328,19 @@
 
 	if (mptcp_create_master_sk(meta_sk, mtreq->mptcp_rem_key,
 				   child_tp->snd_wnd))
+	{
+#ifdef CONFIG_NC_KT_MAGW //09.12
+		mgCheckStat2(sk, child, Fs, "mptcp_check_req_master");
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_CAE);
+#endif
 		return -ENOBUFS;
+	}
+
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	mgCheckStat2(sk, child, Fs, "mptcp_check_req_master");
+	MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MCACK_RX);
+#endif
 
 	child = tcp_sk(child)->mpcb->master_sk;
 	child_tp = tcp_sk(child);
@@ -2085,6 +2363,14 @@
 	 /* Hold when creating the meta-sk in tcp_vX_syn_recv_sock. */
 	sock_put(meta_sk);
 
+
+#ifdef CONFIG_NC_KT_MAGW
+	MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_SUCCESS);
+
+	if(mpcb->sm_ops->on_new_master_session)
+		mpcb->sm_ops->on_new_master_session(mpcb, child);
+#endif
+
 	inet_csk_reqsk_queue_unlink(sk, req, prev);
 	inet_csk_reqsk_queue_removed(sk, req);
 	inet_csk_reqsk_queue_add(sk, req, meta_sk);
@@ -2101,11 +2387,25 @@
 	struct mptcp_request_sock *mtreq = mptcp_rsk(req);
 	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
 	u8 hash_mac_check[20];
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	u8 Fs[4] = {0,};
+
+	mgCheckStat2(meta_sk, child, Fs, "mptcp_check_req_child");
+#endif 
 
 	child_tp->inside_tk_table = 0;
 
-	if (!mopt->join_ack)
+	/* join fail */
+	if (!mopt->join_ack) {
+
+#ifdef CONFIG_NC_KT_MAGW //09.11
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_JAE);
+#endif 
+
+		MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINACKFAIL);
 		goto teardown;
+	}
 
 	mptcp_hmac_sha1((u8 *)&mpcb->mptcp_rem_key,
 			(u8 *)&mpcb->mptcp_loc_key,
@@ -2113,8 +2413,17 @@
 			(u8 *)&mtreq->mptcp_loc_nonce,
 			(u32 *)hash_mac_check);
 
-	if (memcmp(hash_mac_check, (char *)&mopt->mptcp_recv_mac, 20))
+	if (memcmp(hash_mac_check, (char *)&mopt->mptcp_recv_mac, 20)) {
+
+#ifdef CONFIG_NC_KT_MAGW //09.11
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_JAE);
+#endif 
+
+
+		MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINACKMAC);
 		goto teardown;
+	}
 
 	/* Point it to the same struct socket and wq as the meta_sk */
 	sk_set_socket(child, meta_sk->sk_socket);
@@ -2153,12 +2462,22 @@
 
 	child_tp->tsq_flags = 0;
 
+#ifdef CONFIG_NC_KT_MAGW
+	MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MJACK_RX);
+	MAGWS_INC(sock_net(child), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_SUCCESS);
+
+	if(mpcb->sm_ops->on_new_sub_session)
+		mpcb->sm_ops->on_new_sub_session(mpcb, child);
+#endif
+
 	/* Subflows do not use the accept queue, as they
 	 * are attached immediately to the mpcb.
 	 */
 	inet_csk_reqsk_queue_unlink(meta_sk, req, prev);
 	reqsk_queue_removed(&inet_csk(meta_sk)->icsk_accept_queue, req);
 	reqsk_free(req);
+
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINACKRX);
 	return child;
 
 teardown:
@@ -2322,6 +2641,9 @@
 	struct mptcp_request_sock *mtreq = mptcp_rsk(req);
 	struct mptcp_options_received mopt;
 	u8 mptcp_hash_mac[20];
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+	u8 Fs[4] = {0,};
+#endif
 
 	mptcp_init_mp_opt(&mopt);
 	tcp_parse_mptcp_options(skb, &mopt);
@@ -2342,6 +2664,18 @@
 	mtreq->rem_id = mopt.rem_id;
 	mtreq->rcv_low_prio = mopt.low_prio;
 	inet_rsk(req)->saw_mpc = 1;
+
+#ifdef CONFIG_NC_KT_MAGW /* 09.07 */
+
+	mgCheckStat(mpcb->meta_sk, skb, Fs, "mptcp_join_reqsk_init");
+
+	MAGWS_INC(sock_net(mpcb->meta_sk),Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MJSYN_RX);
+	MAGWS_INC(sock_net(mpcb->meta_sk),Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MJSACK_TX);
+	MAGWS_INC(sock_net(mpcb->meta_sk),Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_MP_TRY);
+
+#endif
+
+	MPTCP_INC_STATS(sock_net(mpcb->meta_sk), MPTCP_MIB_JOINSYNRX);
 }
 
 void mptcp_reqsk_init(struct request_sock *req, struct sk_buff *skb)
@@ -2365,6 +2699,9 @@
 	struct mptcp_options_received mopt;
 	__u32 isn = TCP_SKB_CB(skb)->when;
 	bool want_cookie = false;
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+	u8 Fs[4] = {0,};
+#endif
 
 	if ((sysctl_tcp_syncookies == 2 ||
 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
@@ -2391,6 +2728,15 @@
 			    (RTCF_BROADCAST | RTCF_MULTICAST))
 				goto drop;
 
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+			mgCheckStat(sk, skb, Fs, "mptcp_conn_request");
+   
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MCSYN_RX);
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MCSACK_TX);
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_TRY);
+#endif
+
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLEPASSIVE);
 			return tcp_conn_request(&mptcp_request_sock_ops,
 						&mptcp_request_sock_ipv4_ops,
 						sk, skb);
@@ -2403,6 +2749,15 @@
 			if (!ipv6_unicast_destination(skb))
 				goto drop;
 
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+			mgCheckStat(sk, skb, Fs, "mptcp_conn_request");
+   
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MCSYN_RX);
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MCSACK_TX);
+			MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_TRY);
+#endif
+
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLEPASSIVE);
 			return tcp_conn_request(&mptcp6_request_sock_ops,
 						&mptcp_request_sock_ipv6_ops,
 						sk, skb);
@@ -2416,6 +2771,54 @@
 	return 0;
 }
 
+static const struct snmp_mib mptcp_snmp_list[] = {
+	SNMP_MIB_ITEM("MPCapableSYNRX", MPTCP_MIB_MPCAPABLEPASSIVE),
+	SNMP_MIB_ITEM("MPCapableSYNTX", MPTCP_MIB_MPCAPABLEACTIVE),
+	SNMP_MIB_ITEM("MPCapableSYNACKRX", MPTCP_MIB_MPCAPABLEACTIVEACK),
+	SNMP_MIB_ITEM("MPCapableACKRX", MPTCP_MIB_MPCAPABLEPASSIVEACK),
+	SNMP_MIB_ITEM("MPCapableFallbackACK", MPTCP_MIB_MPCAPABLEPASSIVEFALLBACK),
+	SNMP_MIB_ITEM("MPCapableFallbackSYNACK", MPTCP_MIB_MPCAPABLEACTIVEFALLBACK),
+	SNMP_MIB_ITEM("MPCapableRetransFallback", MPTCP_MIB_MPCAPABLERETRANSFALLBACK),
+	SNMP_MIB_ITEM("MPTCPCsumEnabled", MPTCP_MIB_CSUMENABLED),
+	SNMP_MIB_ITEM("MPTCPRetrans", MPTCP_MIB_RETRANSSEGS),
+	SNMP_MIB_ITEM("MPFailRX", MPTCP_MIB_MPFAILRX),
+	SNMP_MIB_ITEM("MPCsumFail", MPTCP_MIB_CSUMFAIL),
+	SNMP_MIB_ITEM("MPFastcloseRX", MPTCP_MIB_FASTCLOSERX),
+	SNMP_MIB_ITEM("MPFastcloseTX", MPTCP_MIB_FASTCLOSETX),
+	SNMP_MIB_ITEM("MPFallbackAckSub", MPTCP_MIB_FBACKSUB),
+	SNMP_MIB_ITEM("MPFallbackAckInit", MPTCP_MIB_FBACKINIT),
+	SNMP_MIB_ITEM("MPFallbackDataSub", MPTCP_MIB_FBDATASUB),
+	SNMP_MIB_ITEM("MPFallbackDataInit", MPTCP_MIB_FBDATAINIT),
+	SNMP_MIB_ITEM("MPRemoveAddrSubDelete", MPTCP_MIB_REMADDRSUB),
+	SNMP_MIB_ITEM("MPJoinNoTokenFound", MPTCP_MIB_JOINNOTOKEN),
+	SNMP_MIB_ITEM("MPJoinAlreadyFallenback", MPTCP_MIB_JOINFALLBACK),
+	SNMP_MIB_ITEM("MPJoinSynTx", MPTCP_MIB_JOINSYNTX),
+	SNMP_MIB_ITEM("MPJoinSynRx", MPTCP_MIB_JOINSYNRX),
+	SNMP_MIB_ITEM("MPJoinSynAckRx", MPTCP_MIB_JOINSYNACKRX),
+	SNMP_MIB_ITEM("MPJoinSynAckHMacFailure", MPTCP_MIB_JOINSYNACKMAC),
+	SNMP_MIB_ITEM("MPJoinAckRx", MPTCP_MIB_JOINACKRX),
+	SNMP_MIB_ITEM("MPJoinAckHMacFailure", MPTCP_MIB_JOINACKMAC),
+	SNMP_MIB_ITEM("MPJoinAckMissing", MPTCP_MIB_JOINACKFAIL),
+	SNMP_MIB_ITEM("MPJoinAckRTO", MPTCP_MIB_JOINACKRTO),
+	SNMP_MIB_ITEM("MPJoinAckRexmit", MPTCP_MIB_JOINACKRXMIT),
+	SNMP_MIB_ITEM("NoDSSInWindow", MPTCP_MIB_NODSSWINDOW),
+	SNMP_MIB_ITEM("DSSNotMatching", MPTCP_MIB_DSSNOMATCH),
+	SNMP_MIB_ITEM("InfiniteMapRx", MPTCP_MIB_INFINITEMAPRX),
+	SNMP_MIB_ITEM("DSSNoMatchTCP", MPTCP_MIB_DSSTCPMISMATCH),
+	SNMP_MIB_ITEM("DSSTrimHead", MPTCP_MIB_DSSTRIMHEAD),
+	SNMP_MIB_ITEM("DSSSplitTail", MPTCP_MIB_DSSSPLITTAIL),
+	SNMP_MIB_ITEM("DSSPurgeOldSubSegs", MPTCP_MIB_PURGEOLD),
+	SNMP_MIB_ITEM("AddAddrRx", MPTCP_MIB_ADDADDRRX),
+	SNMP_MIB_ITEM("AddAddrTx", MPTCP_MIB_ADDADDRTX),
+	SNMP_MIB_ITEM("RemAddrRx", MPTCP_MIB_REMADDRRX),
+	SNMP_MIB_ITEM("RemAddrTx", MPTCP_MIB_REMADDRTX),
+#ifdef CONFIG_NC_KT_MAGW
+	SNMP_MIB_ITEM("DataFinRx", MPTCP_MIB_DATAFINRX),
+	SNMP_MIB_ITEM("DataFinTx", MPTCP_MIB_DATAFINTX),
+#endif
+	SNMP_MIB_SENTINEL
+};
+
 struct workqueue_struct *mptcp_wq;
 EXPORT_SYMBOL(mptcp_wq);
 
@@ -2491,17 +2894,90 @@
 	.release = single_release_net,
 };
 
+static int mptcp_snmp_seq_show(struct seq_file *seq, void *v)
+{
+	struct net *net = seq->private;
+	int i;
+
+	for (i = 0; mptcp_snmp_list[i].name != NULL; i++)
+		seq_printf(seq, "%-32s\t%lu\n", mptcp_snmp_list[i].name,
+			   snmp_fold_field((void __percpu **) net->mptcp.mptcp_statistics,
+				      mptcp_snmp_list[i].entry));
+
+	return 0;
+}
+
+static int mptcp_snmp_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open_net(inode, file, mptcp_snmp_seq_show);
+}
+
+static const struct file_operations mptcp_snmp_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = mptcp_snmp_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release_net,
+};
+
+
 static int mptcp_pm_init_net(struct net *net)
 {
-	if (!proc_create("mptcp", S_IRUGO, net->proc_net, &mptcp_pm_seq_fops))
-		return -ENOMEM;
+
+	if (snmp_mib_init((void __percpu **)net->mptcp.mptcp_statistics, 
+				sizeof(struct mptcp_mib),
+				__alignof__(struct mptcp_mib)) < 0)
+		goto out_mptcp_mibs;
+
+#ifdef CONFIG_PROC_FS
+	net->mptcp.proc_net_mptcp = proc_net_mkdir(net, "mptcp_net", net->proc_net);
+	if (!net->mptcp.proc_net_mptcp)
+		goto out_proc_net_mptcp;
+	if (!proc_create("mptcp", S_IRUGO, net->mptcp.proc_net_mptcp,
+			 &mptcp_pm_seq_fops))
+		goto out_mptcp_net_mptcp;
+	if (!proc_create("snmp", S_IRUGO, net->mptcp.proc_net_mptcp,
+			 &mptcp_snmp_seq_fops))
+		goto out_mptcp_net_snmp;
+#ifdef CONFIG_NC_KT_MAGW
+	if (magw_mib_init_net(net) < 0)
+		goto out_magw_mibs;
+
+	if(magw_proc_init_net(net)<0)
+		goto out_mptcp_net_magw;
+#endif//CONFIG_NC_KT_MAGW
 
 	return 0;
+
+#ifdef CONFIG_NC_KT_MAGW
+out_mptcp_net_magw:
+	magw_mib_exit_net(net);
+out_magw_mibs:
+	remove_proc_entry("snmp", net->mptcp.proc_net_mptcp);
+#endif//CONFIG_NC_KT_MAGW
+out_mptcp_net_snmp:
+	remove_proc_entry("mptcp", net->mptcp.proc_net_mptcp);
+out_mptcp_net_mptcp:
+	remove_proc_subtree("mptcp_net", net->proc_net);
+	net->mptcp.proc_net_mptcp = NULL;
+out_proc_net_mptcp:
+	snmp_mib_free((void __percpu **)net->mptcp.mptcp_statistics);
+#endif
+out_mptcp_mibs:
+	return -ENOMEM;
 }
 
 static void mptcp_pm_exit_net(struct net *net)
 {
-	remove_proc_entry("mptcp", net->proc_net);
+#ifdef CONFIG_NC_KT_MAGW
+	magw_proc_exit_net(net);
+	magw_mib_exit_net(net);
+#endif
+	remove_proc_entry("snmp", net->mptcp.proc_net_mptcp);
+	remove_proc_entry("mptcp", net->mptcp.proc_net_mptcp);
+	remove_proc_subtree("mptcp_net", net->proc_net);
+
+	snmp_mib_free((void __percpu **)net->mptcp.mptcp_statistics);
 }
 
 static struct pernet_operations mptcp_pm_proc_ops = {
@@ -2570,12 +3046,21 @@
 	if (mptcp_register_scheduler(&mptcp_sched_default))
 		goto register_sched_failed;
 
-	pr_info("MPTCP: Stable release v0.89.5");
+#ifdef CONFIG_NC_KT_MAGW
+	if (mptcp_register_session_monitor(&mptcp_sm_default))
+		goto register_sm_failed;
+#endif
+
+	pr_info("MPTCP: Stable release v0.89.5 - for MAGW");
 
 	mptcp_init_failed = false;
 
 	return;
 
+#ifdef CONFIG_NC_KT_MAGW
+register_sm_failed:
+	mptcp_unregister_scheduler(&mptcp_sched_default);
+#endif
 register_sched_failed:
 	mptcp_unregister_path_manager(&mptcp_pm_default);
 register_pm_failed:
diff -ruN c/net/mptcp/mptcp_input.c d/net/mptcp/mptcp_input.c
--- c/net/mptcp/mptcp_input.c	2015-08-04 16:50:26.990553994 +0900
+++ d/net/mptcp/mptcp_input.c	2015-09-12 13:32:10.532648647 +0900
@@ -351,6 +351,7 @@
 		       __func__, csum_fold(csum_tcp), TCP_SKB_CB(last)->seq,
 		       dss_csum_added, overflowed, iter);
 
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_CSUMFAIL);
 		tp->mptcp->send_mp_fail = 1;
 
 		/* map_data_seq is the data-seq number of the
@@ -544,10 +545,13 @@
 		       TCP_SKB_CB(skb)->seq);
 
 		if (!is_master_tp(tp)) {
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_FBDATASUB);
 			mptcp_send_reset(sk);
 			return 1;
 		}
 
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_FBDATAINIT);
+
 		tp->mpcb->infinite_mapping_snd = 1;
 		tp->mpcb->infinite_mapping_rcv = 1;
 
@@ -601,6 +605,7 @@
 		/* Too many packets without a mapping - this subflow is broken */
 		if (!tp->mptcp->mapping_present &&
 		    tp->rcv_nxt - tp->copied_seq > 65536) {
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_NODSSWINDOW);
 			mptcp_send_reset(sk);
 			return 1;
 		}
@@ -637,6 +642,7 @@
 		       sub_seq, tp->mptcp->map_subseq, data_len,
 		       tp->mptcp->map_data_len, mptcp_is_data_fin(skb),
 		       tp->mptcp->map_data_fin);
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DSSNOMATCH);
 		mptcp_send_reset(sk);
 		return 1;
 	}
@@ -669,6 +675,8 @@
 		 * the data and subflow-level
 		 */
 		mptcp_purge_ofo_queue(meta_tp);
+
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_INFINITEMAPRX);
 	}
 
 	/* We are sending mp-fail's and thus are in fallback mode.
@@ -683,8 +691,16 @@
 	}
 
 	/* FIN increased the mapping-length by 1 */
-	if (mptcp_is_data_fin(skb))
+	if (mptcp_is_data_fin(skb)) {
+#ifdef CONFIG_NC_KT_MAGW
+		u8 Fs[4] = {0,};
+		mgCheckStat(sk, skb, Fs, "mptcp_detect_mapping");
+		MAGWS_INC(sock_net(sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_DTFIN_RX);
+
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DATAFINRX); 
+#endif
 		data_len--;
+	}
 
 	/* Subflow-sequences of packet must be
 	 * (at least partially) be part of the DSS-mapping's
@@ -725,6 +741,7 @@
 		       "end_seq %u, tcp_end_seq %u seq %u dfin %u len %u data_len %u"
 		       "copied_seq %u\n", __func__, sub_seq, tcb->end_seq, tcp_end_seq, tcb->seq, mptcp_is_data_fin(skb),
 		       skb->len, data_len, tp->copied_seq);
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DSSTCPMISMATCH);
 		mptcp_send_reset(sk);
 		return 1;
 	}
@@ -795,13 +812,22 @@
 	 */
 	tmp = skb_peek(&sk->sk_receive_queue);
 	if (before(TCP_SKB_CB(tmp)->seq, tp->mptcp->map_subseq) &&
-	    after(TCP_SKB_CB(tmp)->end_seq, tp->mptcp->map_subseq))
+	    after(TCP_SKB_CB(tmp)->end_seq, tp->mptcp->map_subseq)) {
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DSSTRIMHEAD);
 		mptcp_skb_trim_head(tmp, sk, tp->mptcp->map_subseq);
+	}
 
 	/* ... or the new skb (tail) has to be split at the end. */
+#if 0
 	tcp_end_seq = TCP_SKB_CB(skb)->end_seq - (tcp_hdr(skb)->fin ? 1 : 0);
+#else
+	tcp_end_seq = TCP_SKB_CB(skb)->end_seq;
+	if (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)
+		tcp_end_seq--;
+#endif
 	if (after(tcp_end_seq, tp->mptcp->map_subseq + tp->mptcp->map_data_len)) {
 		u32 seq = tp->mptcp->map_subseq + tp->mptcp->map_data_len;
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DSSSPLITTAIL);
 		if (mptcp_skb_split_tail(skb, sk, seq)) { /* Allocation failed */
 			/* TODO : maybe handle this here better.
 			 * We now just force meta-retransmission.
@@ -825,6 +851,7 @@
 			tp->copied_seq = TCP_SKB_CB(tmp1)->end_seq;
 			__skb_unlink(tmp1, &sk->sk_receive_queue);
 
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_PURGEOLD);
 			/* Impossible that we could free skb here, because his
 			 * mapping is known to be valid from previous checks
 			 */
@@ -1155,6 +1182,9 @@
 	struct mptcp_cb *mpcb;
 	struct sock *meta_sk;
 	u32 token;
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+	u8 Fs[4] = {0,};
+#endif
 	struct mp_join *join_opt = mptcp_find_join(skb);
 	if (!join_opt)
 		return 0;
@@ -1166,6 +1196,13 @@
 	token = join_opt->u.syn.token;
 	meta_sk = mptcp_hash_find(dev_net(skb_dst(skb)->dev), token);
 	if (!meta_sk) {
+
+#ifdef CONFIG_NC_KT_MAGW /* 09.12 */
+		mgCheckStat3(skb, Fs, "mptcp_lookup_join");	
+		MAGWS_INC(dev_net(skb_dst(skb)->dev),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_FAIL);
+		MAGWS_INC(dev_net(skb_dst(skb)->dev),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_JSE);
+#endif
+		MPTCP_INC_STATS(dev_net(skb_dst(skb)->dev), MPTCP_MIB_JOINNOTOKEN);
 		mptcp_debug("%s:mpcb not found:%x\n", __func__, token);
 		return -1;
 	}
@@ -1175,7 +1212,15 @@
 		/* We are in fallback-mode on the reception-side -
 		 * no new subflows!
 		 */
+
+#ifdef CONFIG_NC_KT_MAGW /* 09.12 */
+		mgCheckStat(meta_sk, skb, Fs, "mptcp_lookup_join");
+		MAGWS_INC(sock_net(meta_sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(meta_sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_JSE);
+#endif
+
 		sock_put(meta_sk); /* Taken by mptcp_hash_find */
+		MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINFALLBACK);
 		return -1;
 	}
 
@@ -1221,11 +1266,21 @@
 {
 	struct sock *meta_sk;
 	u32 token;
+#ifdef CONFIG_NC_KT_MAGW /* 09.11 */
+	u8 Fs[4] = {0,};
+#endif
 
 	token = mopt->mptcp_rem_token;
 	meta_sk = mptcp_hash_find(net, token);
 	if (!meta_sk) {
+
+#ifdef CONFIG_NC_KT_MAGW /* 09.12 */
+		mgCheckStat3(skb, Fs, "mptcp_lookup_join");	
+		MAGWS_INC(dev_net(skb_dst(skb)->dev),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_FAIL);
+		MAGWS_INC(dev_net(skb_dst(skb)->dev),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_JSE);
+#endif
 		mptcp_debug("%s:mpcb not found:%x\n", __func__, token);
+		MPTCP_INC_STATS(dev_net(skb_dst(skb)->dev), MPTCP_MIB_JOINNOTOKEN);
 		return -1;
 	}
 
@@ -1246,6 +1301,13 @@
 	if (tcp_sk(meta_sk)->mpcb->infinite_mapping_rcv ||
 	    tcp_sk(meta_sk)->mpcb->send_infinite_mapping ||
 	    meta_sk->sk_state == TCP_CLOSE || !tcp_sk(meta_sk)->inside_tk_table) {
+
+#ifdef CONFIG_NC_KT_MAGW /* 09.12 */
+		mgCheckStat(meta_sk, skb, Fs, "mptcp_lookup_join");
+		MAGWS_INC(sock_net(meta_sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_FAIL);
+		MAGWS_INC(sock_net(meta_sk),Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_MP_JSE);
+#endif
+		MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINFALLBACK);
 		bh_unlock_sock(meta_sk);
 		sock_put(meta_sk); /* Taken by mptcp_hash_find */
 		return -1;
@@ -1877,6 +1939,8 @@
 
 	if (mpcb->pm_ops->add_raddr)
 		mpcb->pm_ops->add_raddr(mpcb, &addr, family, port, mpadd->addr_id);
+
+	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_ADDADDRRX);
 }
 
 static void mptcp_handle_rem_addr(const unsigned char *ptr, struct sock *sk)
@@ -1892,7 +1956,11 @@
 		if (mpcb->pm_ops->rem_raddr)
 			mpcb->pm_ops->rem_raddr(mpcb, rem_id);
 		mptcp_send_reset_rem_id(mpcb, rem_id);
+
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_REMADDRSUB);
 	}
+
+	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_REMADDRRX);
 }
 
 static void mptcp_parse_addropt(const struct sk_buff *skb, struct sock *sk)
@@ -1955,8 +2023,18 @@
 	struct mptcp_tcp_sock *mptcp = tcp_sk(sk)->mptcp;
 	struct sock *meta_sk = mptcp_meta_sk(sk);
 	struct mptcp_cb *mpcb = tcp_sk(sk)->mpcb;
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	u8 Fs[4] = {0, };
+#endif
+
 
 	if (unlikely(mptcp->rx_opt.mp_fail)) {
+
+#ifdef CONFIG_NC_KT_MAGW //09.11
+		mgCheckStat2(meta_sk, sk, Fs, "mptcp_mp_fail_rcvd");
+		MAGWS_INC(sock_net(sk), Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_FAIL_RX);
+#endif
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPFAILRX);
 		mptcp->rx_opt.mp_fail = 0;
 
 		if (!th->rst && !mpcb->infinite_mapping_snd) {
@@ -1989,6 +2067,11 @@
 	}
 
 	if (unlikely(mptcp->rx_opt.mp_fclose)) {
+#ifdef CONFIG_NC_KT_MAGW //09.11
+		mgCheckStat2(meta_sk, sk, Fs, "mptcp_mp_fail_rcvd");
+		MAGWS_INC(sock_net(sk), Fs[0],Fs[1],Fs[2],Fs[3], MAGW_MIB_FCLOSE_RX);
+#endif
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_FASTCLOSERX);
 		mptcp->rx_opt.mp_fclose = 0;
 		if (mptcp->rx_opt.mptcp_key != mpcb->mptcp_loc_key)
 			return 0;
@@ -2112,6 +2195,7 @@
 				(u32 *)hash_mac_check);
 		if (memcmp(hash_mac_check,
 			   (char *)&tp->mptcp->rx_opt.mptcp_recv_tmac, 8)) {
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_JOINSYNACKMAC);
 			mptcp_sub_force_close(sk);
 			return 1;
 		}
@@ -2128,7 +2212,10 @@
 				(u8 *)&tp->mptcp->rx_opt.mptcp_recv_nonce,
 				(u32 *)&tp->mptcp->sender_mac[0]);
 
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_JOINSYNACKRX);
 	} else if (mopt->saw_mpc) {
+
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLEACTIVEACK);
 		if (mptcp_create_master_sk(sk, mopt->mptcp_key,
 					   ntohs(tcp_hdr(skb)->window)))
 			return 2;
@@ -2142,6 +2229,9 @@
 		 */
 		tp->mptcp->snt_isn = tp->snd_nxt - 1;
 		tp->mpcb->dss_csum = mopt->dss_csum;
+		if (tp->mpcb->dss_csum)
+			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_CSUMENABLED);
+
 		tp->mptcp->include_mpc = 1;
 
 		/* Ensure that fastopen is handled at the meta-level. */
@@ -2153,6 +2243,8 @@
 		 /* hold in mptcp_inherit_sk due to initialization to 2 */
 		sock_put(sk);
 	} else {
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_MPCAPABLEACTIVEFALLBACK);
+
 		tp->request_mptcp = 0;
 
 		if (tp->inside_tk_table)
diff -ruN c/net/mptcp/mptcp_ipv4.c d/net/mptcp/mptcp_ipv4.c
--- c/net/mptcp/mptcp_ipv4.c	2015-08-04 16:50:26.990553994 +0900
+++ d/net/mptcp/mptcp_ipv4.c	2015-09-22 11:19:21.061310062 +0900
@@ -401,6 +401,8 @@
 		goto error;
 	}
 
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINSYNTX);
+
 	sk_set_socket(sk, meta_sk->sk_socket);
 	sk->sk_wq = meta_sk->sk_wq;
 
diff -ruN c/net/mptcp/mptcp_ipv6.c d/net/mptcp/mptcp_ipv6.c
--- c/net/mptcp/mptcp_ipv6.c	2015-08-04 16:50:26.990553994 +0900
+++ d/net/mptcp/mptcp_ipv6.c	2015-08-04 19:11:38.943228538 +0900
@@ -652,6 +652,8 @@
 		goto error;
 	}
 
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_JOINSYNTX);
+
 	sk_set_socket(sk, meta_sk->sk_socket);
 	sk->sk_wq = meta_sk->sk_wq;
 
diff -ruN c/net/mptcp/mptcp_kt01.c d/net/mptcp/mptcp_kt01.c
--- c/net/mptcp/mptcp_kt01.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/mptcp_kt01.c	2015-09-30 02:32:18.829610565 +0900
@@ -0,0 +1,601 @@
+/* MPTCP Scheduler module selector. Highly inspired by tcp_cong.c */
+
+#include <linux/module.h>
+#include <net/mptcp.h>
+
+
+
+
+int sysctl_magw_kt01_rttthresh __read_mostly = 30;
+int sysctl_magw_kt01_weight __read_mostly = 70;
+int sysctl_magw_kt01_debug __read_mostly = 0;
+
+int g_kt01_path1_log = 0;
+int g_kt01_path2_log = 0;
+
+
+#define kt01_debug(fmt, args...)		        \
+	do {								        \
+		if (unlikely(sysctl_magw_kt01_debug))	\
+			pr_err(__FILE__ ": " fmt, ##args);	\
+	} while (0)
+
+
+
+/* Same as defsched_priv from mptcp_sched.c */
+struct defsched_priv {
+	u32	last_rbuf_opti;
+};
+
+/* Same as defsched_get_priv from mptcp_sched.c */
+static struct defsched_priv *defsched_get_priv(const struct tcp_sock *tp)
+{
+	return (struct defsched_priv *)&tp->mptcp->mptcp_sched[0];
+}
+
+/* If the sub-socket sk available to send the skb? */
+static bool mptcp_is_available(struct sock *sk, struct sk_buff *skb,
+			       bool zero_wnd_test)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	unsigned int mss_now, space, in_flight;
+
+	/* Set of states for which we are allowed to send data */
+	if (!mptcp_sk_can_send(sk))
+		return false;
+
+	/* We do not send data on this subflow unless it is
+	 * fully established, i.e. the 4th ack has been received.
+	 */
+	if (tp->mptcp->pre_established)
+		return false;
+
+	if (tp->pf)
+		return false;
+
+	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss) {
+		/* If SACK is disabled, and we got a loss, TCP does not exit
+		 * the loss-state until something above high_seq has been acked.
+		 * (see tcp_try_undo_recovery)
+		 *
+		 * high_seq is the snd_nxt at the moment of the RTO. As soon
+		 * as we have an RTO, we won't push data on the subflow.
+		 * Thus, snd_una can never go beyond high_seq.
+		 */
+		if (!tcp_is_reno(tp))
+			return false;
+		else if (tp->snd_una != tp->high_seq)
+			return false;
+	}
+
+	if (!tp->mptcp->fully_established) {
+		/* Make sure that we send in-order data */
+		if (skb && tp->mptcp->second_packet &&
+		    tp->mptcp->last_end_data_seq != TCP_SKB_CB(skb)->seq)
+			return false;
+	}
+
+	/* If TSQ is already throttling us, do not send on this subflow. When
+	 * TSQ gets cleared the subflow becomes eligible again.
+	 */
+	if (test_bit(TSQ_THROTTLED, &tp->tsq_flags))
+		return false;
+
+	in_flight = tcp_packets_in_flight(tp);
+	/* Not even a single spot in the cwnd */
+	if (in_flight >= tp->snd_cwnd)
+		return false;
+
+	/* Now, check if what is queued in the subflow's send-queue
+	 * already fills the cwnd.
+	 */
+	space = (tp->snd_cwnd - in_flight) * tp->mss_cache;
+
+#if 0
+	kt01_debug("KT[%s] sub_pi[%d] skb->len[%d] mss_now[%d] in_flight[%u] space[%u]\n", 
+				__func__, tp->mptcp->path_index, skb->len, tcp_current_mss(sk),
+				in_flight, space);
+#endif
+
+	if (tp->write_seq - tp->snd_nxt > space)
+		return false;
+
+	if (zero_wnd_test && !before(tp->write_seq, tcp_wnd_end(tp)))
+		return false;
+
+	mss_now = tcp_current_mss(sk);
+
+	/* Don't send on this subflow if we bypass the allowed send-window at
+	 * the per-subflow level. Similar to tcp_snd_wnd_test, but manually
+	 * calculated end_seq (because here at this point end_seq is still at
+	 * the meta-level).
+	 */
+	if (skb && !zero_wnd_test &&
+	    after(tp->write_seq + min(skb->len, mss_now), tcp_wnd_end(tp)))
+		return false;
+
+	return true;
+}
+
+/* Are we not allowed to reinject this skb on tp? */
+static int mptcp_dont_reinject_skb(struct tcp_sock *tp, struct sk_buff *skb)
+{
+	/* If the skb has already been enqueued in this sk, try to find
+	 * another one.
+	 */
+	return skb &&
+		/* Has the skb already been enqueued into this subsocket? */
+		mptcp_pi_to_flag(tp->mptcp->path_index) & TCP_SKB_CB(skb)->path_mask;
+}
+
+/* This is the scheduler. This function decides on which flow to send
+ * a given MSS. If all subflows are found to be busy, NULL is returned
+ * The flow is selected based on the shortest RTT.
+ * If all paths have full cong windows, we simply return NULL.
+ *
+ * Additionally, this function is aware of the backup-subflows.
+ */
+static struct sock *kt01_get_avail_subflow(struct sock *meta_sk,
+					  struct sk_buff *skb,
+					  bool zero_wnd_test)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sock *sk, *bestsk = NULL, *lowpriosk = NULL, *backupsk = NULL;
+	u32 min_time_to_peer = 0xffffffff, lowprio_min_time_to_peer = 0xffffffff;
+	int cnt_backups = 0;
+
+
+	/* if there is only one subflow, bypass the scheduling function */
+	if (mpcb->cnt_subflows == 1) {
+		bestsk = (struct sock *)mpcb->connection_list;
+		if (!mptcp_is_available(bestsk, skb, zero_wnd_test))
+			bestsk = NULL;
+		return bestsk;
+	}
+
+	/* Answer data_fin on same subflow!!! */
+	if (meta_sk->sk_shutdown & RCV_SHUTDOWN &&
+	    skb && mptcp_is_data_fin(skb)) {
+		mptcp_for_each_sk(mpcb, sk) {
+			if (tcp_sk(sk)->mptcp->path_index == mpcb->dfin_path_index &&
+			    mptcp_is_available(sk, skb, zero_wnd_test))
+				return sk;
+		}
+	}
+
+	/* First, find the best subflow */
+	mptcp_for_each_sk(mpcb, sk) {
+		struct tcp_sock *tp = tcp_sk(sk);
+
+		if (tp->mptcp->rcv_low_prio || tp->mptcp->low_prio)
+			cnt_backups++;
+
+#if 0
+			kt01_debug("KT[%s] pi[%d] srtt[%u] cwnd[%u,%u,%u] prior[%d] ssthresh[%u] [%d]\n", 
+						__func__, 
+						tp->mptcp->path_index, 
+						tp->srtt,
+						tp->snd_cwnd,
+						tp->snd_cwnd_cnt,
+						tp->snd_cwnd_used,
+						tp->prior_cwnd,
+						tp->snd_ssthresh,
+						sysctl_magw_kt01_rttthresh
+						);
+#endif
+
+		if ((tp->mptcp->rcv_low_prio || tp->mptcp->low_prio) &&
+		    tp->srtt < lowprio_min_time_to_peer) {
+			if (!mptcp_is_available(sk, skb, zero_wnd_test))
+				continue;
+
+			if (mptcp_dont_reinject_skb(tp, skb)) {
+				backupsk = sk;
+				continue;
+			}
+
+			lowprio_min_time_to_peer = tp->srtt;
+			lowpriosk = sk;
+		} else if (!(tp->mptcp->rcv_low_prio || tp->mptcp->low_prio) &&
+			   tp->srtt < min_time_to_peer) {
+			if (!mptcp_is_available(sk, skb, zero_wnd_test))
+				continue;
+
+			if (mptcp_dont_reinject_skb(tp, skb)) {
+				backupsk = sk;
+				continue;
+			}
+
+			min_time_to_peer = tp->srtt;
+			bestsk = sk;
+		}
+	}
+
+
+	if (mpcb->cnt_established == cnt_backups && lowpriosk) {
+		sk = lowpriosk;
+#if 0
+		struct tcp_sock *tp = tcp_sk(sk);
+		kt01_debug("KT[%s] lowpriosk pi[%d] srtt[%u]\n", 
+						__func__, tp->mptcp->path_index, tp->srtt);
+#endif
+
+	} else if (bestsk) {
+		sk = bestsk;
+#if 0
+		struct tcp_sock *tp = tcp_sk(sk);
+		kt01_debug("KT[%s] BEST SK pi[%d] srtt[%u]\n", 
+						__func__, tp->mptcp->path_index, tp->srtt);
+#endif
+
+	} else if (backupsk) {
+		/* It has been sent on all subflows once - let's give it a
+		 * chance again by restarting its pathmask.
+		 */
+		if (skb)
+			TCP_SKB_CB(skb)->path_mask = 0;
+		sk = backupsk;
+	}
+
+	return sk;
+}
+
+static struct sk_buff *mptcp_rcv_buf_optimization(struct sock *sk, int penal)
+{
+	struct sock *meta_sk;
+	struct tcp_sock *tp = tcp_sk(sk), *tp_it;
+	struct sk_buff *skb_head;
+	struct defsched_priv *dsp = defsched_get_priv(tp);
+
+	if (tp->mpcb->cnt_subflows == 1)
+		return NULL;
+
+	meta_sk = mptcp_meta_sk(sk);
+	skb_head = tcp_write_queue_head(meta_sk);
+
+	if (!skb_head || skb_head == tcp_send_head(meta_sk))
+		return NULL;
+
+	/* If penalization is optional (coming from mptcp_next_segment() and
+	 * We are not send-buffer-limited we do not penalize. The retransmission
+	 * is just an optimization to fix the idle-time due to the delay before
+	 * we wake up the application.
+	 */
+	if (!penal && sk_stream_memory_free(meta_sk))
+		goto retrans;
+
+	/* Only penalize again after an RTT has elapsed */
+	if (tcp_time_stamp - dsp->last_rbuf_opti < tp->srtt >> 3)
+		goto retrans;
+
+	/* Half the cwnd of the slow flow */
+	mptcp_for_each_tp(tp->mpcb, tp_it) {
+		if (tp_it != tp &&
+		    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+			if (tp->srtt < tp_it->srtt && inet_csk((struct sock *)tp_it)->icsk_ca_state == TCP_CA_Open) {
+				u32 prior_cwnd = tp_it->snd_cwnd;
+
+				tp_it->snd_cwnd = max(tp_it->snd_cwnd >> 1U, 1U);
+
+				/* If in slow start, do not reduce the ssthresh */
+				if (prior_cwnd >= tp_it->snd_ssthresh)
+					tp_it->snd_ssthresh = max(tp_it->snd_ssthresh >> 1U, 2U);
+
+				dsp->last_rbuf_opti = tcp_time_stamp;
+			}
+			break;
+		}
+	}
+
+retrans:
+
+	/* Segment not yet injected into this path? Take it!!! */
+	if (!(TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp->mptcp->path_index))) {
+		bool do_retrans = false;
+		mptcp_for_each_tp(tp->mpcb, tp_it) {
+			if (tp_it != tp &&
+			    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+				if (tp_it->snd_cwnd <= 4) {
+					do_retrans = true;
+					break;
+				}
+
+				if (4 * tp->srtt >= tp_it->srtt) {
+					do_retrans = false;
+					break;
+				} else {
+					do_retrans = true;
+				}
+			}
+		}
+
+		if (do_retrans && mptcp_is_available(sk, skb_head, false))
+			return skb_head;
+	}
+	return NULL;
+}
+
+/* Returns the next segment to be sent from the mptcp meta-queue.
+ * (chooses the reinject queue if any segment is waiting in it, otherwise,
+ * chooses the normal write queue).
+ * Sets *@reinject to 1 if the returned segment comes from the
+ * reinject queue. Sets it to 0 if it is the regular send-head of the meta-sk,
+ * and sets it to -1 if it is a meta-level retransmission to optimize the
+ * receive-buffer.
+ */
+static struct sk_buff *__kt01_next_segment(struct sock *meta_sk, int *reinject)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sk_buff *skb = NULL;
+
+	*reinject = 0;
+
+	/* If we are in fallback-mode, just take from the meta-send-queue */
+	if (mpcb->infinite_mapping_snd || mpcb->send_infinite_mapping)
+		return tcp_send_head(meta_sk);
+
+	skb = skb_peek(&mpcb->reinject_queue);
+
+	if (skb) {
+		*reinject = 1;
+	} else {
+		skb = tcp_send_head(meta_sk);
+
+		if (!skb && meta_sk->sk_socket &&
+		    test_bit(SOCK_NOSPACE, &meta_sk->sk_socket->flags) &&
+		    sk_stream_wspace(meta_sk) < sk_stream_min_wspace(meta_sk)) {
+			struct sock *subsk = kt01_get_avail_subflow(meta_sk, NULL,
+								   false);
+			if (!subsk)
+				return NULL;
+
+			skb = mptcp_rcv_buf_optimization(subsk, 0);
+			if (skb)
+				*reinject = -1;
+		}
+	}
+	return skb;
+}
+
+static struct sk_buff *kt01_next_segment(struct sock *meta_sk,
+					  int *reinject,
+					  struct sock **subsk,
+					  unsigned int *limit)
+{
+	struct sk_buff *skb = __kt01_next_segment(meta_sk, reinject);
+	unsigned int mss_now;
+	struct tcp_sock *subtp;
+	u16 gso_max_segs;
+	u32 max_len, max_segs, window, needed;
+
+	unsigned char split = 0;
+
+	/* As we set it, we have to reset it as well. */
+	*limit = 0;
+
+	if (!skb)
+		return NULL;
+
+	*subsk = kt01_get_avail_subflow(meta_sk, skb, false);
+	if (!*subsk)
+		return NULL;
+
+	subtp = tcp_sk(*subsk);
+	mss_now = tcp_current_mss(*subsk);
+
+#if 0
+	kt01_debug("KT[%s] sub_pi[%d] srtt[%u] skb->len[%d] mss_now[%d]\n", 
+				__func__, subtp->mptcp->path_index, subtp->srtt, skb->len, mss_now);
+#endif
+
+	if (!*reinject && unlikely(!tcp_snd_wnd_test(tcp_sk(meta_sk), skb, mss_now))) {
+		skb = mptcp_rcv_buf_optimization(*subsk, 1);
+		if (skb)
+			*reinject = -1;
+		else
+			return NULL;
+	}
+
+#if 0
+	/* No splitting required, as we will only send one single segment */
+	if (skb->len <= mss_now)
+		return skb;
+#endif
+
+	/* KT01 Algorithm.. */
+	if(subtp->srtt > sysctl_magw_kt01_rttthresh)
+	{
+		if(sysctl_magw_kt01_weight == 0)
+			return NULL;
+
+		split = (subtp->snd_cwnd*sysctl_magw_kt01_weight)/100;
+		if(split==0) split=1;
+
+		*limit = split * mss_now;
+
+		if (skb->len <= mss_now)
+			*limit = 0;
+
+
+		if(subtp->mptcp->path_index == 1 && g_kt01_path1_log == 0)
+		{
+			g_kt01_path1_log = 1;
+			kt01_debug("KT[%s] pi(%d) SLOW PATH START[%u>%u]:skb->len[%d] mss[%u] cwnd[%d-->%d (%d/100)]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt01_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd, split, sysctl_magw_kt01_weight
+				);
+		}
+		else if(subtp->mptcp->path_index == 2 && g_kt01_path2_log == 0)
+		{
+			g_kt01_path2_log = 1;
+			kt01_debug("KT[%s] pi(%d) SLOW PATH START[%u>%u]:skb->len[%d] mss[%u] cwnd[%d-->%d (%d/100)]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt01_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd, split, sysctl_magw_kt01_weight
+				);
+		}
+#if 0
+		else
+		{
+			kt01_debug("KT[%s] pi(%d) SLOW PATH START[%u>%u]:skb->len[%d] mss[%u] cwnd[%d-->%d (%d/100)]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt01_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd, split, sysctl_magw_kt01_weight
+				);
+		}
+#endif
+
+		return skb;
+	}
+	else
+	{
+		 if(subtp->mptcp->path_index == 1 && g_kt01_path1_log == 1)
+		 {
+			g_kt01_path1_log = 0;
+			kt01_debug("KT[%s] pi(%d) SLOW PATH END [%u>%u]:skb->len[%d] mss[%u] cwnd[%d]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt01_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd
+				);
+		 }
+		 else if(subtp->mptcp->path_index == 2 && g_kt01_path2_log == 1)
+		 {
+			g_kt01_path2_log = 0;
+			kt01_debug("KT[%s] pi(%d) SLOW PATH END [%u>%u]:skb->len[%d] mss[%u] cwnd[%d]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt01_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd
+				);
+		 }
+	}
+
+
+#if 1
+	/* No splitting required, as we will only send one single segment */
+	if (skb->len <= mss_now)
+		return skb;
+#endif
+
+	/* The following is similar to tcp_mss_split_point, but
+	 * we do not care about nagle, because we will anyways
+	 * use TCP_NAGLE_PUSH, which overrides this.
+	 *
+	 * So, we first limit according to the cwnd/gso-size and then according
+	 * to the subflow's window.
+	 */
+
+	gso_max_segs = (*subsk)->sk_gso_max_segs;
+	if (!gso_max_segs) /* No gso supported on the subflow's NIC */
+		gso_max_segs = 1;
+	max_segs = min_t(unsigned int, tcp_cwnd_test(subtp, skb), gso_max_segs);
+	if (!max_segs)
+		return NULL;
+
+	max_len = mss_now * max_segs;
+	window = tcp_wnd_end(subtp) - subtp->write_seq;
+
+	needed = min(skb->len, window);
+	if (max_len <= skb->len)
+		/* Take max_win, which is actually the cwnd/gso-size */
+		*limit = max_len;
+	else
+		/* Or, take the window */
+		*limit = needed;
+
+	return skb;
+}
+
+/* Same as defsched_init from mptcp_sched.c */
+static void defsched_init(struct sock *sk)
+{
+	struct defsched_priv *dsp = defsched_get_priv(tcp_sk(sk));
+
+	dsp->last_rbuf_opti = tcp_time_stamp;
+}
+
+
+/* ------------------------------------------------------------------------- */
+static struct mptcp_sched_ops mptcp_sched_kt01 = {
+	.get_subflow = kt01_get_avail_subflow,
+	.next_segment = kt01_next_segment,
+	.init = defsched_init,
+	.flags= MPTCP_SCHED_NON_RESTRICTED,
+	.name = "kt01",
+	.owner = THIS_MODULE,
+};
+
+/* ------------------------------------------------------------------------- */
+static struct ctl_table magw_kt01_table[] = {
+	{
+		.procname = "magw_kt01_rttthresh",
+		.data = &sysctl_magw_kt01_rttthresh,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname = "magw_kt01_weight",
+		.data = &sysctl_magw_kt01_weight,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname = "magw_kt01_debug",
+		.data = &sysctl_magw_kt01_debug,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+};
+
+/* ------------------------------------------------------------------------- */
+struct ctl_table_header *mptcp_sysctl_kt01;
+
+static int __init kt01_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct defsched_priv) > MPTCP_SCHED_SIZE);
+
+	pr_crit("[%s] KT01:TRY REGISTER\n", __func__);
+
+	mptcp_sysctl_kt01 = register_net_sysctl(&init_net, 
+			                                "net/mptcp", 
+											magw_kt01_table);
+
+	if (!mptcp_sysctl_kt01)
+		goto register_sysctl_failed;
+
+	if (mptcp_register_scheduler(&mptcp_sched_kt01))
+		goto register_sched_failed;
+
+	pr_crit("[%s] KT01:REGISTERED\n", __func__);
+
+	return 0;
+
+register_sched_failed:
+	unregister_net_sysctl_table(mptcp_sysctl_kt01);
+register_sysctl_failed:
+	return -1;
+}
+
+static void __exit kt01_unregister(void)
+{
+	mptcp_unregister_scheduler(&mptcp_sched_kt01);
+	unregister_net_sysctl_table(mptcp_sysctl_kt01);
+}
+
+module_init(kt01_register);
+module_exit(kt01_unregister);
+
+MODULE_AUTHOR("YOU DON'T KNOW ME");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("SCHEDULER FOR KT MAGW AL01");
+MODULE_VERSION("0.01");
+
+
diff -ruN c/net/mptcp/mptcp_kt02.c d/net/mptcp/mptcp_kt02.c
--- c/net/mptcp/mptcp_kt02.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/mptcp_kt02.c	2015-10-01 20:34:35.868150574 +0900
@@ -0,0 +1,415 @@
+/* MPTCP Scheduler module selector. Highly inspired by mptcp_rr.c */
+
+#include <linux/module.h>
+#include <net/mptcp.h>
+
+
+/* if set to 1, the scheduler tries to fill the congestion-window on all subflows */
+static bool cwnd_limited __read_mostly = 0;
+
+/* "The number of consecutive segments that are part of a burst */
+int sysctl_magw_kt02_num_segments __read_mostly = 1;
+int sysctl_magw_kt02_p1_weight    __read_mostly = 5;
+int sysctl_magw_kt02_p2_weight    __read_mostly = 1;
+int sysctl_magw_kt02_debug 		  __read_mostly = 0;
+
+
+
+#define kt02_debug(fmt, args...)		        \
+	do {								        \
+		if (unlikely(sysctl_magw_kt02_debug))	\
+			pr_err(__FILE__ ": " fmt, ##args);	\
+	} while (0)
+
+
+
+/* ------------------------------------------------------------------------- */
+struct rrsched_priv {
+	unsigned char quota;
+};
+
+/* ------------------------------------------------------------------------- */
+static struct rrsched_priv *rrsched_get_priv(const struct tcp_sock *tp)
+{
+	return (struct rrsched_priv *)&tp->mptcp->mptcp_sched[0];
+}
+
+/* ------------------------------------------------------------------------- */
+/* If the sub-socket sk available to send the skb? */
+static bool kt02_rr_is_available(struct sock *sk, struct sk_buff *skb,
+				  bool zero_wnd_test, bool cwnd_test)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	unsigned int space, in_flight;
+
+	/* Set of states for which we are allowed to send data */
+	if (!mptcp_sk_can_send(sk))
+		return false;
+
+	/* We do not send data on this subflow unless it is
+	 * fully established, i.e. the 4th ack has been received.
+	 */
+	if (tp->mptcp->pre_established)
+		return false;
+
+	if (tp->pf)
+		return false;
+
+	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss) {
+		/* If SACK is disabled, and we got a loss, TCP does not exit
+		 * the loss-state until something above high_seq has been acked.
+		 * (see tcp_try_undo_recovery)
+		 *
+		 * high_seq is the snd_nxt at the moment of the RTO. As soon
+		 * as we have an RTO, we won't push data on the subflow.
+		 * Thus, snd_una can never go beyond high_seq.
+		 */
+		if (!tcp_is_reno(tp))
+			return false;
+		else if (tp->snd_una != tp->high_seq)
+			return false;
+	}
+
+	if (!tp->mptcp->fully_established) {
+		/* Make sure that we send in-order data */
+		if (skb && tp->mptcp->second_packet &&
+		    tp->mptcp->last_end_data_seq != TCP_SKB_CB(skb)->seq)
+			return false;
+	}
+
+	if (!cwnd_test)
+		goto zero_wnd_test;
+
+	in_flight = tcp_packets_in_flight(tp);
+	/* Not even a single spot in the cwnd */
+	if (in_flight >= tp->snd_cwnd)
+		return false;
+
+	/* Now, check if what is queued in the subflow's send-queue
+	 * already fills the cwnd.
+	 */
+	space = (tp->snd_cwnd - in_flight) * tp->mss_cache;
+
+	if (tp->write_seq - tp->snd_nxt > space)
+		return false;
+
+zero_wnd_test:
+	if (zero_wnd_test && !before(tp->write_seq, tcp_wnd_end(tp)))
+		return false;
+
+	return true;
+}
+
+/* ------------------------------------------------------------------------- */
+/* Are we not allowed to reinject this skb on tp? */
+static int mptcp_rr_dont_reinject_skb(struct tcp_sock *tp, struct sk_buff *skb)
+{
+	/* If the skb has already been enqueued in this sk, try to find
+	 * another one.
+	 */
+	return skb &&
+		/* Has the skb already been enqueued into this subsocket? */
+		mptcp_pi_to_flag(tp->mptcp->path_index) & TCP_SKB_CB(skb)->path_mask;
+}
+
+/* ------------------------------------------------------------------------- */
+/* We just look for any subflow that is available */
+static struct sock *kt02_get_available_subflow(struct sock *meta_sk,
+					     struct sk_buff *skb,
+					     bool zero_wnd_test)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sock *sk, *bestsk = NULL, *backupsk = NULL;
+
+	/* Answer data_fin on same subflow!!! */
+	if (meta_sk->sk_shutdown & RCV_SHUTDOWN &&
+	    skb && mptcp_is_data_fin(skb)) {
+		mptcp_for_each_sk(mpcb, sk) {
+			if (tcp_sk(sk)->mptcp->path_index == mpcb->dfin_path_index &&
+			    kt02_rr_is_available(sk, skb, zero_wnd_test, true))
+				return sk;
+		}
+	}
+
+	/* First, find the best subflow */
+	mptcp_for_each_sk(mpcb, sk) {
+		struct tcp_sock *tp = tcp_sk(sk);
+
+		if (!kt02_rr_is_available(sk, skb, zero_wnd_test, true))
+			continue;
+
+		if (mptcp_rr_dont_reinject_skb(tp, skb)) {
+			backupsk = sk;
+			continue;
+		}
+
+		bestsk = sk;
+	}
+
+	if (bestsk) {
+		sk = bestsk;
+	} else if (backupsk) {
+		/* It has been sent on all subflows once - let's give it a
+		 * chance again by restarting its pathmask.
+		 */
+		if (skb)
+			TCP_SKB_CB(skb)->path_mask = 0;
+		sk = backupsk;
+	}
+
+	return sk;
+}
+
+/* ------------------------------------------------------------------------- */
+/* Returns the next segment to be sent from the mptcp meta-queue.
+ * (chooses the reinject queue if any segment is waiting in it, otherwise,
+ * chooses the normal write queue).
+ * Sets *@reinject to 1 if the returned segment comes from the
+ * reinject queue. Sets it to 0 if it is the regular send-head of the meta-sk,
+ * and sets it to -1 if it is a meta-level retransmission to optimize the
+ * receive-buffer.
+ */
+static struct sk_buff *__kt02_next_segment(struct sock *meta_sk, int *reinject)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sk_buff *skb = NULL;
+
+	*reinject = 0;
+
+	/* If we are in fallback-mode, just take from the meta-send-queue */
+	if (mpcb->infinite_mapping_snd || mpcb->send_infinite_mapping)
+		return tcp_send_head(meta_sk);
+
+	skb = skb_peek(&mpcb->reinject_queue);
+
+	if (skb)
+		*reinject = 1;
+	else
+		skb = tcp_send_head(meta_sk);
+	return skb;
+}
+
+/* ------------------------------------------------------------------------- */
+static struct sk_buff *kt02_next_segment(struct sock *meta_sk,
+					     int *reinject,
+					     struct sock **subsk,
+					     unsigned int *limit)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct mptcp_tcp_sock *mptcp = NULL;
+	struct sock *sk_it, *choose_sk = NULL;
+	struct sk_buff *skb = __kt02_next_segment(meta_sk, reinject);
+	//unsigned char split = sysctl_magw_kt02_num_segments;
+	//
+	unsigned char num_segments = 0;
+	unsigned char split = 0;
+	unsigned char iter = 0, full_subs = 0;
+
+	/* As we set it, we have to reset it as well. */
+	*limit = 0;
+
+	if (!skb)
+		return NULL;
+
+	if (*reinject) {
+		*subsk = kt02_get_available_subflow(meta_sk, skb, false);
+		if (!*subsk)
+			return NULL;
+
+		return skb;
+	}
+
+retry:
+
+	/* First, we look for a subflow who is currently being used */
+	mptcp_for_each_sk(mpcb, sk_it) {
+		struct tcp_sock *tp_it = tcp_sk(sk_it);
+		struct rrsched_priv *rsp = rrsched_get_priv(tp_it);
+
+		if (!kt02_rr_is_available(sk_it, skb, false, cwnd_limited))
+			continue;
+
+		iter++;
+
+		/* re-calculate num_segments */
+		mptcp = tp_it->mptcp;
+		if(mptcp && mptcp->path_index==1)
+		{
+			num_segments = sysctl_magw_kt02_num_segments *
+				 ( (sysctl_magw_kt02_p1_weight<=0)?1:sysctl_magw_kt02_p1_weight);
+		}
+		else if(mptcp && mptcp->path_index==2)
+		{
+			num_segments = sysctl_magw_kt02_num_segments *
+				 ( (sysctl_magw_kt02_p2_weight<=0)?1:sysctl_magw_kt02_p2_weight);
+		}
+		else
+		{
+			num_segments = sysctl_magw_kt02_num_segments;
+		}
+
+
+		/* Is this subflow currently being used? */
+		if (rsp->quota > 0 && rsp->quota < num_segments) {
+			split = num_segments - rsp->quota;
+			choose_sk = sk_it;
+			goto found;
+		}
+
+		/* Or, it's totally unused */
+		if (!rsp->quota) {
+			split = num_segments;
+			choose_sk = sk_it;
+		}
+
+		/* Or, it must then be fully used  */
+		if (rsp->quota >= num_segments)
+			full_subs++;
+	}
+
+	/* All considered subflows have a full quota, and we considered at
+	 * least one.
+	 */
+	if (iter && iter == full_subs) {
+		/* So, we restart this round by setting quota to 0 and retry
+		 * to find a subflow.
+		 */
+		mptcp_for_each_sk(mpcb, sk_it) {
+			struct tcp_sock *tp_it = tcp_sk(sk_it);
+			struct rrsched_priv *rsp = rrsched_get_priv(tp_it);
+
+			if (!kt02_rr_is_available(sk_it, skb, false, cwnd_limited))
+				continue;
+
+			rsp->quota = 0;
+		}
+
+		goto retry;
+	}
+
+found:
+	if (choose_sk) {
+		unsigned int mss_now;
+		struct tcp_sock *choose_tp = tcp_sk(choose_sk);
+		struct rrsched_priv *rsp = rrsched_get_priv(choose_tp);
+
+		if (!kt02_rr_is_available(choose_sk, skb, false, true))
+			return NULL;
+
+		*subsk = choose_sk;
+		mss_now = tcp_current_mss(*subsk);
+		*limit = split * mss_now;
+
+		if (skb->len > mss_now)
+			rsp->quota += DIV_ROUND_UP(skb->len, mss_now);
+		else
+			rsp->quota++;
+
+		kt02_debug("KT[%s] pi(%d) quota[%d], skb->len[%d] mss[%u] split[%d] limit[%d] w[%d,%d]\n", 
+			__func__, choose_tp->mptcp->path_index, 
+			rsp->quota,
+			skb->len, mss_now, split, *limit,
+			sysctl_magw_kt02_p1_weight, sysctl_magw_kt02_p2_weight
+			);
+
+
+		return skb;
+	}
+
+	return NULL;
+}
+
+
+/* ------------------------------------------------------------------------- */
+static void kt02_sched_init(struct sock *sk)
+{
+	struct rrsched_priv *rsp = rrsched_get_priv(tcp_sk(sk));
+	rsp->quota = 0;
+}
+
+/* ------------------------------------------------------------------------- */
+struct mptcp_sched_ops mptcp_sched_kt02 = {
+	.get_subflow  = kt02_get_available_subflow,
+	.next_segment = kt02_next_segment,
+	.init         = kt02_sched_init,
+	.flags= MPTCP_SCHED_NON_RESTRICTED,
+	.name = "kt02",
+	.owner = THIS_MODULE,
+};
+
+/* ------------------------------------------------------------------------- */
+static struct ctl_table magw_kt02_table[] = {
+    {
+        .procname = "magw_kt02_num_segments",
+        .data = &sysctl_magw_kt02_num_segments,
+        .maxlen = sizeof(int),
+        .mode = 0644,
+        .proc_handler = &proc_dointvec
+    },
+    {
+        .procname = "magw_kt02_p1_weight",
+        .data = &sysctl_magw_kt02_p1_weight,
+        .maxlen = sizeof(int),
+        .mode = 0644,
+        .proc_handler = &proc_dointvec
+    },
+    {
+        .procname = "magw_kt02_p2_weight",
+        .data = &sysctl_magw_kt02_p2_weight,
+        .maxlen = sizeof(int),
+        .mode = 0644,
+        .proc_handler = &proc_dointvec
+    },
+    {
+        .procname = "magw_kt02_debug",
+        .data = &sysctl_magw_kt02_debug,
+        .maxlen = sizeof(int),
+        .mode = 0644,
+        .proc_handler = &proc_dointvec
+    },
+};
+
+/* ------------------------------------------------------------------------- */
+struct ctl_table_header *mptcp_sysctl_kt02;
+
+static int __init kt02_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct rrsched_priv) > MPTCP_SCHED_SIZE);
+
+	pr_crit("[%s] KT03:TRY REGISTER\n", __func__);
+
+    mptcp_sysctl_kt02 = register_net_sysctl(&init_net,
+                                            "net/mptcp",
+                                            magw_kt02_table);
+
+    if (!mptcp_sysctl_kt02)
+        goto register_sysctl_failed;
+
+
+	if (mptcp_register_scheduler(&mptcp_sched_kt02))
+		goto register_sched_failed;
+
+	pr_crit("[%s] KT03:REGISTERED\n", __func__);
+
+	return 0;
+
+register_sched_failed:
+    unregister_net_sysctl_table(mptcp_sysctl_kt02);
+register_sysctl_failed:
+    return -1;
+}
+
+/* ------------------------------------------------------------------------- */
+static void kt02_unregister(void)
+{
+	mptcp_unregister_scheduler(&mptcp_sched_kt02);
+	unregister_net_sysctl_table(mptcp_sysctl_kt02);
+}
+
+module_init(kt02_register);
+module_exit(kt02_unregister);
+
+MODULE_AUTHOR("DON'T YOU KNOW ME STILL?");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("SCHEDULER FOR KT MAGW AL03");
+MODULE_VERSION("0.01");
diff -ruN c/net/mptcp/mptcp_kt04.c d/net/mptcp/mptcp_kt04.c
--- c/net/mptcp/mptcp_kt04.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/mptcp_kt04.c	2015-09-30 02:04:06.000830054 +0900
@@ -0,0 +1,555 @@
+/* MPTCP Scheduler module selector. Highly inspired by tcp_cong.c */
+
+#include <linux/module.h>
+#include <net/mptcp.h>
+
+
+int sysctl_magw_kt04_rttthresh __read_mostly = 30;
+int sysctl_magw_kt04_weight __read_mostly = 70;
+int sysctl_magw_kt04_debug __read_mostly = 0;
+
+#define kt04_debug(fmt, args...)		        \
+	do {								        \
+		if (unlikely(sysctl_magw_kt04_debug))	\
+			pr_err(__FILE__ ": " fmt, ##args);	\
+	} while (0)
+
+
+/* ------------------------------------------------------------------------- */
+/* Same as defsched_priv from mptcp_sched.c */
+struct defsched_priv {
+	u32	last_rbuf_opti;
+};
+
+/* ------------------------------------------------------------------------- */
+/* Same as defsched_get_priv from mptcp_sched.c */
+static struct defsched_priv *defsched_get_priv(const struct tcp_sock *tp)
+{
+	return (struct defsched_priv *)&tp->mptcp->mptcp_sched[0];
+}
+
+/* ------------------------------------------------------------------------- */
+/* If the sub-socket sk available to send the skb? */
+static bool mptcp_is_available(struct sock *sk, struct sk_buff *skb,
+			       bool zero_wnd_test)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	unsigned int mss_now, space, in_flight;
+
+	/* Set of states for which we are allowed to send data */
+	if (!mptcp_sk_can_send(sk))
+		return false;
+
+	/* We do not send data on this subflow unless it is
+	 * fully established, i.e. the 4th ack has been received.
+	 */
+	if (tp->mptcp->pre_established)
+		return false;
+
+	if (tp->pf)
+		return false;
+
+	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss) {
+		/* If SACK is disabled, and we got a loss, TCP does not exit
+		 * the loss-state until something above high_seq has been acked.
+		 * (see tcp_try_undo_recovery)
+		 *
+		 * high_seq is the snd_nxt at the moment of the RTO. As soon
+		 * as we have an RTO, we won't push data on the subflow.
+		 * Thus, snd_una can never go beyond high_seq.
+		 */
+		if (!tcp_is_reno(tp))
+			return false;
+		else if (tp->snd_una != tp->high_seq)
+			return false;
+	}
+
+	if (!tp->mptcp->fully_established) {
+		/* Make sure that we send in-order data */
+		if (skb && tp->mptcp->second_packet &&
+		    tp->mptcp->last_end_data_seq != TCP_SKB_CB(skb)->seq)
+			return false;
+	}
+
+	/* If TSQ is already throttling us, do not send on this subflow. When
+	 * TSQ gets cleared the subflow becomes eligible again.
+	 */
+	if (test_bit(TSQ_THROTTLED, &tp->tsq_flags))
+		return false;
+
+	in_flight = tcp_packets_in_flight(tp);
+	/* Not even a single spot in the cwnd */
+	if (in_flight >= tp->snd_cwnd)
+		return false;
+
+	/* Now, check if what is queued in the subflow's send-queue
+	 * already fills the cwnd.
+	 */
+	space = (tp->snd_cwnd - in_flight) * tp->mss_cache;
+
+#if 0
+	kt04_debug("KT[%s] sub_pi[%d] skb->len[%d] mss_now[%d] in_flight[%u] space[%u]\n", 
+				__func__, tp->mptcp->path_index, skb->len, tcp_current_mss(sk),
+				in_flight, space);
+#endif
+
+	if (tp->write_seq - tp->snd_nxt > space)
+		return false;
+
+	if (zero_wnd_test && !before(tp->write_seq, tcp_wnd_end(tp)))
+		return false;
+
+	mss_now = tcp_current_mss(sk);
+
+	/* Don't send on this subflow if we bypass the allowed send-window at
+	 * the per-subflow level. Similar to tcp_snd_wnd_test, but manually
+	 * calculated end_seq (because here at this point end_seq is still at
+	 * the meta-level).
+	 */
+	if (skb && !zero_wnd_test &&
+	    after(tp->write_seq + min(skb->len, mss_now), tcp_wnd_end(tp)))
+		return false;
+
+	return true;
+}
+
+/* ------------------------------------------------------------------------- */
+/* Are we not allowed to reinject this skb on tp? */
+static int mptcp_dont_reinject_skb(struct tcp_sock *tp, struct sk_buff *skb)
+{
+	/* If the skb has already been enqueued in this sk, try to find
+	 * another one.
+	 */
+	return skb &&
+		/* Has the skb already been enqueued into this subsocket? */
+		mptcp_pi_to_flag(tp->mptcp->path_index) & TCP_SKB_CB(skb)->path_mask;
+}
+
+/* ------------------------------------------------------------------------- */
+/* This is the scheduler. This function decides on which flow to send
+ * a given MSS. If all subflows are found to be busy, NULL is returned
+ * The flow is selected based on the shortest RTT.
+ * If all paths have full cong windows, we simply return NULL.
+ *
+ * Additionally, this function is aware of the backup-subflows.
+ */
+static struct sock *kt04_get_avail_subflow(struct sock *meta_sk,
+					  struct sk_buff *skb,
+					  bool zero_wnd_test)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sock *sk, *bestsk = NULL, *lowpriosk = NULL, *backupsk = NULL;
+	u32 min_time_to_peer = 0xffffffff, lowprio_min_time_to_peer = 0xffffffff;
+	int cnt_backups = 0;
+
+
+	/* if there is only one subflow, bypass the scheduling function */
+	if (mpcb->cnt_subflows == 1) {
+		bestsk = (struct sock *)mpcb->connection_list;
+		if (!mptcp_is_available(bestsk, skb, zero_wnd_test))
+			bestsk = NULL;
+		return bestsk;
+	}
+
+	/* Answer data_fin on same subflow!!! */
+	if (meta_sk->sk_shutdown & RCV_SHUTDOWN &&
+	    skb && mptcp_is_data_fin(skb)) {
+		mptcp_for_each_sk(mpcb, sk) {
+			if (tcp_sk(sk)->mptcp->path_index == mpcb->dfin_path_index &&
+			    mptcp_is_available(sk, skb, zero_wnd_test))
+				return sk;
+		}
+	}
+
+	/* First, find the best subflow */
+	mptcp_for_each_sk(mpcb, sk) {
+		struct tcp_sock *tp = tcp_sk(sk);
+
+		if (tp->mptcp->rcv_low_prio || tp->mptcp->low_prio)
+			cnt_backups++;
+
+#if 0
+			kt04_debug("KT[%s] pi[%d] srtt[%u] cwnd[%u,%u,%u] prior[%d] ssthresh[%u] [%d]\n", 
+						__func__, 
+						tp->mptcp->path_index, 
+						tp->srtt,
+						tp->snd_cwnd,
+						tp->snd_cwnd_cnt,
+						tp->snd_cwnd_used,
+						tp->prior_cwnd,
+						tp->snd_ssthresh,
+						sysctl_magw_kt04_rttthresh
+						);
+#endif
+
+		if ((tp->mptcp->rcv_low_prio || tp->mptcp->low_prio) &&
+		    tp->srtt < lowprio_min_time_to_peer) {
+			if (!mptcp_is_available(sk, skb, zero_wnd_test))
+				continue;
+
+			if (mptcp_dont_reinject_skb(tp, skb)) {
+				backupsk = sk;
+				continue;
+			}
+
+			lowprio_min_time_to_peer = tp->srtt;
+			lowpriosk = sk;
+		} else if (!(tp->mptcp->rcv_low_prio || tp->mptcp->low_prio) &&
+			   tp->srtt < min_time_to_peer) {
+			if (!mptcp_is_available(sk, skb, zero_wnd_test))
+				continue;
+
+			if (mptcp_dont_reinject_skb(tp, skb)) {
+				backupsk = sk;
+				continue;
+			}
+
+			min_time_to_peer = tp->srtt;
+			bestsk = sk;
+		}
+	}
+
+
+	if (mpcb->cnt_established == cnt_backups && lowpriosk) {
+		sk = lowpriosk;
+#if 0
+		struct tcp_sock *tp = tcp_sk(sk);
+		kt04_debug("KT[%s] lowpriosk pi[%d] srtt[%u]\n", 
+						__func__, tp->mptcp->path_index, tp->srtt);
+#endif
+
+	} else if (bestsk) {
+		sk = bestsk;
+#if 0
+		struct tcp_sock *tp = tcp_sk(sk);
+		kt04_debug("KT[%s] BEST SK pi[%d] srtt[%u]\n", 
+						__func__, tp->mptcp->path_index, tp->srtt);
+#endif
+
+	} else if (backupsk) {
+		/* It has been sent on all subflows once - let's give it a
+		 * chance again by restarting its pathmask.
+		 */
+		if (skb)
+			TCP_SKB_CB(skb)->path_mask = 0;
+		sk = backupsk;
+	}
+
+	return sk;
+}
+
+/* ------------------------------------------------------------------------- */
+static struct sk_buff *mptcp_rcv_buf_optimization(struct sock *sk, int penal)
+{
+	struct sock *meta_sk;
+	struct tcp_sock *tp = tcp_sk(sk), *tp_it;
+	struct sk_buff *skb_head;
+	struct defsched_priv *dsp = defsched_get_priv(tp);
+
+	if (tp->mpcb->cnt_subflows == 1)
+		return NULL;
+
+	meta_sk = mptcp_meta_sk(sk);
+	skb_head = tcp_write_queue_head(meta_sk);
+
+	if (!skb_head || skb_head == tcp_send_head(meta_sk))
+		return NULL;
+
+	/* If penalization is optional (coming from mptcp_next_segment() and
+	 * We are not send-buffer-limited we do not penalize. The retransmission
+	 * is just an optimization to fix the idle-time due to the delay before
+	 * we wake up the application.
+	 */
+	if (!penal && sk_stream_memory_free(meta_sk))
+		goto retrans;
+
+	/* Only penalize again after an RTT has elapsed */
+	if (tcp_time_stamp - dsp->last_rbuf_opti < tp->srtt >> 3)
+		goto retrans;
+
+	/* Half the cwnd of the slow flow */
+	mptcp_for_each_tp(tp->mpcb, tp_it) {
+		if (tp_it != tp &&
+		    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+			if (tp->srtt < tp_it->srtt && inet_csk((struct sock *)tp_it)->icsk_ca_state == TCP_CA_Open) {
+				u32 prior_cwnd = tp_it->snd_cwnd;
+
+				tp_it->snd_cwnd = max(tp_it->snd_cwnd >> 1U, 1U);
+
+				/* If in slow start, do not reduce the ssthresh */
+				if (prior_cwnd >= tp_it->snd_ssthresh)
+					tp_it->snd_ssthresh = max(tp_it->snd_ssthresh >> 1U, 2U);
+
+				dsp->last_rbuf_opti = tcp_time_stamp;
+			}
+			break;
+		}
+	}
+
+retrans:
+
+	/* Segment not yet injected into this path? Take it!!! */
+	if (!(TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp->mptcp->path_index))) {
+		bool do_retrans = false;
+		mptcp_for_each_tp(tp->mpcb, tp_it) {
+			if (tp_it != tp &&
+			    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+				if (tp_it->snd_cwnd <= 4) {
+					do_retrans = true;
+					break;
+				}
+
+				if (4 * tp->srtt >= tp_it->srtt) {
+					do_retrans = false;
+					break;
+				} else {
+					do_retrans = true;
+				}
+			}
+		}
+
+		if (do_retrans && mptcp_is_available(sk, skb_head, false))
+			return skb_head;
+	}
+	return NULL;
+}
+
+/* ------------------------------------------------------------------------- */
+/* Returns the next segment to be sent from the mptcp meta-queue.
+ * (chooses the reinject queue if any segment is waiting in it, otherwise,
+ * chooses the normal write queue).
+ * Sets *@reinject to 1 if the returned segment comes from the
+ * reinject queue. Sets it to 0 if it is the regular send-head of the meta-sk,
+ * and sets it to -1 if it is a meta-level retransmission to optimize the
+ * receive-buffer.
+ */
+static struct sk_buff *__kt04_next_segment(struct sock *meta_sk, int *reinject)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sk_buff *skb = NULL;
+
+	*reinject = 0;
+
+	/* If we are in fallback-mode, just take from the meta-send-queue */
+	if (mpcb->infinite_mapping_snd || mpcb->send_infinite_mapping)
+		return tcp_send_head(meta_sk);
+
+	skb = skb_peek(&mpcb->reinject_queue);
+
+	if (skb) {
+		*reinject = 1;
+	} else {
+		skb = tcp_send_head(meta_sk);
+
+		if (!skb && meta_sk->sk_socket &&
+		    test_bit(SOCK_NOSPACE, &meta_sk->sk_socket->flags) &&
+		    sk_stream_wspace(meta_sk) < sk_stream_min_wspace(meta_sk)) {
+			struct sock *subsk = kt04_get_avail_subflow(meta_sk, NULL,
+								   false);
+			if (!subsk)
+				return NULL;
+
+			skb = mptcp_rcv_buf_optimization(subsk, 0);
+			if (skb)
+				*reinject = -1;
+		}
+	}
+	return skb;
+}
+
+/* ------------------------------------------------------------------------- */
+static struct sk_buff *kt04_next_segment(struct sock *meta_sk,
+					  int *reinject,
+					  struct sock **subsk,
+					  unsigned int *limit)
+{
+	struct sk_buff *skb = __kt04_next_segment(meta_sk, reinject);
+	unsigned int mss_now;
+	struct tcp_sock *subtp;
+	u16 gso_max_segs;
+	u32 max_len, max_segs, window, needed;
+
+	unsigned char split = 0;
+
+	/* As we set it, we have to reset it as well. */
+	*limit = 0;
+
+	if (!skb)
+		return NULL;
+
+	*subsk = kt04_get_avail_subflow(meta_sk, skb, false);
+	if (!*subsk)
+		return NULL;
+
+	subtp = tcp_sk(*subsk);
+	mss_now = tcp_current_mss(*subsk);
+
+#if 0
+	kt04_debug("KT[%s] sub_pi[%d] srtt[%u] skb->len[%d] mss_now[%d]\n", 
+				__func__, subtp->mptcp->path_index, subtp->srtt, skb->len, mss_now);
+#endif
+
+	if (!*reinject && unlikely(!tcp_snd_wnd_test(tcp_sk(meta_sk), skb, mss_now))) {
+		skb = mptcp_rcv_buf_optimization(*subsk, 1);
+		if (skb)
+			*reinject = -1;
+		else
+			return NULL;
+	}
+
+#if 0
+	/* No splitting required, as we will only send one single segment */
+	if (skb->len <= mss_now)
+		return skb;
+#endif
+
+	/* KT02 Algorithm.. */
+	if(subtp->srtt > sysctl_magw_kt04_rttthresh)
+	{
+		if(sysctl_magw_kt04_weight == 0)
+			return NULL;
+
+		split = (subtp->snd_cwnd*sysctl_magw_kt04_weight)/100;
+		if(split==0) split=1;
+
+		*limit = split * mss_now;
+
+		if (skb->len <= mss_now)
+			*limit = 0;
+
+
+		kt04_debug("KT[%s] pi(%d) SLOW PATH [%u>%u]:skb->len[%d] mss[%u] cwnd[%d-->%d (%d/100)]\n", 
+				__func__, subtp->mptcp->path_index, 
+				subtp->srtt, sysctl_magw_kt04_rttthresh,
+				skb->len, mss_now,
+				subtp->snd_cwnd, split, sysctl_magw_kt04_weight
+				);
+
+		return skb;
+	}
+
+
+#if 1
+	/* No splitting required, as we will only send one single segment */
+	if (skb->len <= mss_now)
+		return skb;
+#endif
+
+	/* The following is similar to tcp_mss_split_point, but
+	 * we do not care about nagle, because we will anyways
+	 * use TCP_NAGLE_PUSH, which overrides this.
+	 *
+	 * So, we first limit according to the cwnd/gso-size and then according
+	 * to the subflow's window.
+	 */
+
+	gso_max_segs = (*subsk)->sk_gso_max_segs;
+	if (!gso_max_segs) /* No gso supported on the subflow's NIC */
+		gso_max_segs = 1;
+	max_segs = min_t(unsigned int, tcp_cwnd_test(subtp, skb), gso_max_segs);
+	if (!max_segs)
+		return NULL;
+
+	max_len = mss_now * max_segs;
+	window = tcp_wnd_end(subtp) - subtp->write_seq;
+
+	needed = min(skb->len, window);
+	if (max_len <= skb->len)
+		/* Take max_win, which is actually the cwnd/gso-size */
+		*limit = max_len;
+	else
+		/* Or, take the window */
+		*limit = needed;
+
+	return skb;
+}
+
+/* ------------------------------------------------------------------------- */
+/* Same as defsched_init from mptcp_sched.c */
+static void defsched_init(struct sock *sk)
+{
+	struct defsched_priv *dsp = defsched_get_priv(tcp_sk(sk));
+
+	dsp->last_rbuf_opti = tcp_time_stamp;
+}
+
+
+/* ------------------------------------------------------------------------- */
+static struct mptcp_sched_ops mptcp_sched_kt04 = {
+	.get_subflow = kt04_get_avail_subflow,
+	.next_segment = kt04_next_segment,
+	.init = defsched_init,
+	.flags= MPTCP_SCHED_NON_RESTRICTED,
+	.name = "kt04",
+	.owner = THIS_MODULE,
+};
+
+/* ------------------------------------------------------------------------- */
+static struct ctl_table magw_kt04_table[] = {
+	{
+		.procname = "magw_kt04_rttthresh",
+		.data = &sysctl_magw_kt04_rttthresh,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname = "magw_kt04_weight",
+		.data = &sysctl_magw_kt04_weight,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+	{
+		.procname = "magw_kt04_debug",
+		.data = &sysctl_magw_kt04_debug,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec
+	},
+};
+
+/* ------------------------------------------------------------------------- */
+struct ctl_table_header *mptcp_sysctl_kt04;
+
+static int __init kt04_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct defsched_priv) > MPTCP_SCHED_SIZE);
+
+	pr_crit("[%s] KT02:TRY REGISTER\n", __func__);
+
+	mptcp_sysctl_kt04 = register_net_sysctl(&init_net, 
+			                                "net/mptcp", 
+											magw_kt04_table);
+
+	if (!mptcp_sysctl_kt04)
+		goto register_sysctl_failed;
+
+	if (mptcp_register_scheduler(&mptcp_sched_kt04))
+		goto register_sched_failed;
+
+	pr_crit("[%s] KT02:REGISTERED\n", __func__);
+
+	return 0;
+
+register_sched_failed:
+	unregister_net_sysctl_table(mptcp_sysctl_kt04);
+register_sysctl_failed:
+	return -1;
+}
+
+static void __exit kt04_unregister(void)
+{
+	mptcp_unregister_scheduler(&mptcp_sched_kt04);
+	unregister_net_sysctl_table(mptcp_sysctl_kt04);
+}
+
+module_init(kt04_register);
+module_exit(kt04_unregister);
+
+MODULE_AUTHOR("DON'T YOU KNOW ME?");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("SCHEDULER FOR KT MAGW AL01");
+MODULE_VERSION("0.01");
+
+
diff -ruN c/net/mptcp/mptcp_output.c d/net/mptcp/mptcp_output.c
--- c/net/mptcp/mptcp_output.c	2015-08-04 16:50:26.991553981 +0900
+++ d/net/mptcp/mptcp_output.c	2015-09-12 13:32:09.797658881 +0900
@@ -746,6 +746,9 @@
 						TCP_SKB_CB(skb)->end_seq -
 						TCP_SKB_CB(skb)->seq);
 			tcp_event_new_data_sent(meta_sk, skb);
+#ifdef CONFIG_NC_KT_MAGW 
+            nc_inc_tx_stat(subtp, skb->len);
+#endif
 		}
 
 		tcp_minshall_update(meta_tp, mss_now, skb);
@@ -1079,6 +1082,8 @@
 			       sizeof(mpadd->u.v6.addr));
 			ptr += MPTCP_SUB_LEN_ADD_ADDR6_ALIGN >> 2;
 		}
+
+		MPTCP_INC_STATS(sock_net((struct sock *)tp), MPTCP_MIB_ADDADDRTX);
 	}
 	if (unlikely(OPTION_REMOVE_ADDR & opts->mptcp_options)) {
 		struct mp_remove_addr *mprem = (struct mp_remove_addr *)ptr;
@@ -1105,6 +1110,8 @@
 		}
 
 		ptr += len_align >> 2;
+
+		MPTCP_INC_STATS(sock_net((struct sock *)tp), MPTCP_MIB_REMADDRTX);
 	}
 	if (unlikely(OPTION_MP_FAIL & opts->mptcp_options)) {
 		struct mp_fail *mpfail = (struct mp_fail *)ptr;
@@ -1157,6 +1164,9 @@
 	struct tcp_sock *meta_tp = tcp_sk(meta_sk);
 	struct sk_buff *skb = tcp_write_queue_tail(meta_sk);
 	int mss_now;
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	u8 Fs[4] = {0,};
+#endif
 
 	if ((1 << meta_sk->sk_state) & (TCPF_CLOSE_WAIT | TCPF_LAST_ACK))
 		meta_tp->mpcb->passive_close = 1;
@@ -1188,6 +1198,14 @@
 		TCP_SKB_CB(skb)->mptcp_flags |= MPTCPHDR_FIN;
 		tcp_queue_skb(meta_sk, skb);
 	}
+#ifdef CONFIG_NC_KT_MAGW
+
+	mgCheckStat2(meta_sk, meta_sk, Fs, "mptcp_send_fin");
+
+	MAGWS_INC(sock_net(meta_sk), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_DTFIN_TX); 
+
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_DATAFINTX);
+#endif
 	__tcp_push_pending_frames(meta_sk, mss_now, TCP_NAGLE_OFF);
 }
 
@@ -1196,6 +1214,9 @@
 	struct tcp_sock *meta_tp = tcp_sk(meta_sk);
 	struct mptcp_cb *mpcb = meta_tp->mpcb;
 	struct sock *sk = NULL, *sk_it = NULL, *tmpsk;
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	u8 Fs[4] = {0,};
+#endif
 
 	if (!mpcb->cnt_subflows)
 		return;
@@ -1239,10 +1260,17 @@
 	if (!in_serving_softirq())
 		local_bh_enable();
 
+#ifdef CONFIG_NC_KT_MAGW //09.11
+	mgCheckStat2(meta_sk, sk, Fs, "mptcp_send_active_reset");
+	MAGWS_INC(sock_net(meta_sk), Fs[0],Fs[1],Fs[2],Fs[3],MAGW_MIB_FCLOSE_TX); 
+#endif //CONFIG_NC_KT_MAGW
+
 	tcp_send_ack(sk);
 	inet_csk_reset_keepalive_timer(sk, inet_csk(sk)->icsk_rto);
 
 	meta_tp->send_mp_fclose = 1;
+
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_FASTCLOSETX);
 }
 
 static void mptcp_ack_retransmit_timer(struct sock *sk)
@@ -1258,6 +1286,7 @@
 		tp->retrans_stamp = tcp_time_stamp ? : 1;
 
 	if (tcp_write_timeout(sk)) {
+		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_JOINACKRTO);
 		tp->mptcp->pre_established = 0;
 		sk_stop_timer(sk, &tp->mptcp->mptcp_ack_timer);
 		tp->send_active_reset(sk, GFP_ATOMIC);
@@ -1275,6 +1304,8 @@
 	skb_reserve(skb, MAX_TCP_HEADER);
 	tcp_init_nondata_skb(skb, tp->snd_una, TCPHDR_ACK);
 
+	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_JOINACKRXMIT);
+
 	TCP_SKB_CB(skb)->when = tcp_time_stamp;
 	if (tcp_transmit_skb(sk, skb, 0, GFP_ATOMIC) > 0) {
 		/* Retransmission failed because of local congestion,
@@ -1391,7 +1422,7 @@
 	TCP_SKB_CB(skb)->when = tcp_time_stamp;
 
 	/* Update global TCP statistics. */
-	TCP_INC_STATS(sock_net(meta_sk), TCP_MIB_RETRANSSEGS);
+	MPTCP_INC_STATS(sock_net(meta_sk), MPTCP_MIB_RETRANSSEGS);
 
 	/* Diff to tcp_retransmit_skb */
 
diff -ruN c/net/mptcp/mptcp_redundant.c d/net/mptcp/mptcp_redundant.c
--- c/net/mptcp/mptcp_redundant.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/mptcp_redundant.c	2015-09-19 01:57:53.651857885 +0900
@@ -0,0 +1,584 @@
+/* MPTCP Scheduler for redundant transmission.
+ *
+ * The scheduler will transmit information replicated through all the available
+ * active (non-backup) subflows. When backup subflows are used no replication
+ * is performed
+ *
+ * The code is highly inspired in mptcp_sched.c
+ *
+ * Design:
+ * Christian Pinedo <christian.pinedo@ehu.es> <chr.pinedo@gmail.com>
+ * Igor Lopez <igor.lopez@ehu.es>
+ *
+ * Implementation:
+ * Christian Pinedo <christian.pinedo@ehu.es> <chr.pinedo@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/module.h>	/* Needed by all modules */
+#include <net/mptcp.h>
+
+/* Same as mptcp_dss_len from mptcp_output.c */
+static const int mptcp_dss_len = MPTCP_SUB_LEN_DSS_ALIGN +
+				 MPTCP_SUB_LEN_ACK_ALIGN +
+				 MPTCP_SUB_LEN_SEQ_ALIGN;
+
+/* Same as defsched_priv from mptcp_sched.c */
+struct defsched_priv {
+	u32	last_rbuf_opti;
+};
+
+/* Same as defsched_get_priv from mptcp_sched.c */
+static struct defsched_priv *defsched_get_priv(const struct tcp_sock *tp)
+{
+	return (struct defsched_priv *)&tp->mptcp->mptcp_sched[0];
+}
+
+/* Same as defsched_init from mptcp_sched.c */
+static void defsched_init(struct sock *sk)
+{
+	struct defsched_priv *dsp = defsched_get_priv(tcp_sk(sk));
+
+	dsp->last_rbuf_opti = tcp_time_stamp;
+}
+
+/* Same as mptcp_is_def_unavailable from mptcp_sched.c */
+static bool mptcp_is_def_unavailable(struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	/* Set of states for which we are allowed to send data */
+	if (!mptcp_sk_can_send(sk))
+		return true;
+
+	/* We do not send data on this subflow unless it is
+	 * fully established, i.e. the 4th ack has been received.
+	 */
+	if (tp->mptcp->pre_established)
+		return true;
+
+	if (tp->pf)
+		return true;
+
+	return false;
+}
+
+/* Same as mptcp_is_temp_unavailable from mptcp_sched.c */
+static bool mptcp_is_temp_unavailable(struct sock *sk,
+				      const struct sk_buff *skb,
+				      bool zero_wnd_test)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	unsigned int mss_now, space, in_flight;
+
+	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss) {
+		/* If SACK is disabled, and we got a loss, TCP does not exit
+		 * the loss-state until something above high_seq has been
+		 * acked. (see tcp_try_undo_recovery)
+		 *
+		 * high_seq is the snd_nxt at the moment of the RTO. As soon
+		 * as we have an RTO, we won't push data on the subflow.
+		 * Thus, snd_una can never go beyond high_seq.
+		 */
+		if (!tcp_is_reno(tp))
+			return true;
+		else if (tp->snd_una != tp->high_seq)
+			return true;
+	}
+
+	if (!tp->mptcp->fully_established) {
+		/* Make sure that we send in-order data */
+		if (skb && tp->mptcp->second_packet &&
+		    tp->mptcp->last_end_data_seq != TCP_SKB_CB(skb)->seq)
+			return true;
+	}
+
+	/* If TSQ is already throttling us, do not send on this subflow. When
+	 * TSQ gets cleared the subflow becomes eligible again.
+	 */
+	if (test_bit(TSQ_THROTTLED, &tp->tsq_flags))
+		return true;
+
+	in_flight = tcp_packets_in_flight(tp);
+	/* Not even a single spot in the cwnd */
+	if (in_flight >= tp->snd_cwnd)
+		return true;
+
+	/* Now, check if what is queued in the subflow's send-queue
+	 * already fills the cwnd.
+	 */
+	space = (tp->snd_cwnd - in_flight) * tp->mss_cache;
+
+	if (tp->write_seq - tp->snd_nxt > space)
+		return true;
+
+	if (zero_wnd_test && !before(tp->write_seq, tcp_wnd_end(tp)))
+		return true;
+
+	mss_now = tcp_current_mss(sk);
+
+	/* Don't send on this subflow if we bypass the allowed send-window at
+	 * the per-subflow level. Similar to tcp_snd_wnd_test, but manually
+	 * calculated end_seq (because here at this point end_seq is still at
+	 * the meta-level).
+	 */
+	if (skb && !zero_wnd_test &&
+	    after(tp->write_seq + min(skb->len, mss_now), tcp_wnd_end(tp)))
+		return true;
+
+	return false;
+}
+
+/* Same as mptcp_is_available from mptcp_sched.c */
+static bool mptcp_is_available(struct sock *sk, const struct sk_buff *skb,
+			       bool zero_wnd_test)
+{
+	return !mptcp_is_def_unavailable(sk) &&
+	       !mptcp_is_temp_unavailable(sk, skb, zero_wnd_test);
+}
+
+/* Same as mptcp_dont_reinject_skb from mptcp_sched.c */
+static int mptcp_dont_reinject_skb(const struct tcp_sock *tp, const struct sk_buff *skb)
+{
+	/* If the skb has already been enqueued in this sk, try to find
+	 * another one.
+	 */
+	return skb &&
+		/* Has the skb already been enqueued into this subsocket? */
+		mptcp_pi_to_flag(tp->mptcp->path_index) & TCP_SKB_CB(skb)->path_mask;
+}
+
+/* Same as subflow_is_backup from mptcp_sched.c */
+static bool subflow_is_backup(const struct tcp_sock *tp)
+{
+	return tp->mptcp->rcv_low_prio || tp->mptcp->low_prio;
+}
+
+/* Same as subflow_is_active from mptcp_sched.c */
+static bool subflow_is_active(const struct tcp_sock *tp)
+{
+	return !tp->mptcp->rcv_low_prio && !tp->mptcp->low_prio;
+}
+
+/* Same as subflow_is_active from mptcp_sched.c */
+static struct sock
+*get_subflow_from_selectors(struct mptcp_cb *mpcb, struct sk_buff *skb,
+			    bool (*selector)(const struct tcp_sock *),
+			    bool zero_wnd_test, bool *force)
+{
+	struct sock *bestsk = NULL;
+	u32 min_srtt = 0xffffffff;
+	bool found_unused = false;
+	bool found_unused_una = false;
+	struct sock *sk;
+
+	mptcp_for_each_sk(mpcb, sk) {
+		struct tcp_sock *tp = tcp_sk(sk);
+		bool unused = false;
+
+		/* First, we choose only the wanted sks */
+		if (!(*selector)(tp))
+			continue;
+
+		if (!mptcp_dont_reinject_skb(tp, skb))
+			unused = true;
+		else if (found_unused)
+			/* If a unused sk was found previously, we continue -
+			 * no need to check used sks anymore.
+			 */
+			continue;
+
+		if (mptcp_is_def_unavailable(sk))
+			continue;
+
+		if (mptcp_is_temp_unavailable(sk, skb, zero_wnd_test)) {
+			if (unused)
+				found_unused_una = true;
+			continue;
+		}
+
+		if (unused) {
+			if (!found_unused) {
+				/* It's the first time we encounter an unused
+				 * sk - thus we reset the bestsk (which might
+				 * have been set to a used sk).
+				 */
+				min_srtt = 0xffffffff;
+				bestsk = NULL;
+			}
+			found_unused = true;
+		}
+
+		if (tp->srtt_us < min_srtt) {
+			min_srtt = tp->srtt_us;
+			bestsk = sk;
+		}
+	}
+
+	if (bestsk) {
+		/* The force variable is used to mark the returned sk as
+		 * previously used or not-used.
+		 */
+		if (found_unused)
+			*force = true;
+		else
+			*force = false;
+	} else {
+		/* The force variable is used to mark if there are temporally
+		 * unavailable not-used sks.
+		 */
+		if (found_unused_una)
+			*force = true;
+		else
+			*force = false;
+	}
+
+	return bestsk;
+}
+
+/* Same as mptcp_rcv_buf_optimization from mptcp_sched.c */
+static struct sk_buff *mptcp_rcv_buf_optimization(struct sock *sk, int penal)
+{
+	struct sock *meta_sk;
+	const struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_sock *tp_it;
+	struct sk_buff *skb_head;
+	struct defsched_priv *dsp = defsched_get_priv(tp);
+
+	if (tp->mpcb->cnt_subflows == 1)
+		return NULL;
+
+	meta_sk = mptcp_meta_sk(sk);
+	skb_head = tcp_write_queue_head(meta_sk);
+
+	if (!skb_head || skb_head == tcp_send_head(meta_sk))
+		return NULL;
+
+	/* If penalization is optional (coming from mptcp_next_segment() and
+	 * We are not send-buffer-limited we do not penalize. The retransmission
+	 * is just an optimization to fix the idle-time due to the delay before
+	 * we wake up the application.
+	 */
+	if (!penal && sk_stream_memory_free(meta_sk))
+		goto retrans;
+
+	/* Only penalize again after an RTT has elapsed */
+	if (tcp_time_stamp - dsp->last_rbuf_opti < usecs_to_jiffies(tp->srtt_us >> 3))
+		goto retrans;
+
+	/* Half the cwnd of the slow flow */
+	mptcp_for_each_tp(tp->mpcb, tp_it) {
+		if (tp_it != tp &&
+		    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+			if (tp->srtt_us < tp_it->srtt_us && inet_csk((struct sock *)tp_it)->icsk_ca_state == TCP_CA_Open) {
+				u32 prior_cwnd = tp_it->snd_cwnd;
+
+				tp_it->snd_cwnd = max(tp_it->snd_cwnd >> 1U, 1U);
+
+				/* If in slow start, do not reduce the ssthresh */
+				if (prior_cwnd >= tp_it->snd_ssthresh)
+					tp_it->snd_ssthresh = max(tp_it->snd_ssthresh >> 1U, 2U);
+
+				dsp->last_rbuf_opti = tcp_time_stamp;
+			}
+			break;
+		}
+	}
+
+retrans:
+
+	/* Segment not yet injected into this path? Take it!!! */
+	if (!(TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp->mptcp->path_index))) {
+		bool do_retrans = false;
+		mptcp_for_each_tp(tp->mpcb, tp_it) {
+			if (tp_it != tp &&
+			    TCP_SKB_CB(skb_head)->path_mask & mptcp_pi_to_flag(tp_it->mptcp->path_index)) {
+				if (tp_it->snd_cwnd <= 4) {
+					do_retrans = true;
+					break;
+				}
+
+				if (4 * tp->srtt_us >= tp_it->srtt_us) {
+					do_retrans = false;
+					break;
+				} else {
+					do_retrans = true;
+				}
+			}
+		}
+
+		if (do_retrans && mptcp_is_available(sk, skb_head, false))
+			return skb_head;
+	}
+	return NULL;
+}
+
+/* Same as get_available_subflow from mptcp_sched.c */
+static struct sock *get_available_subflow(struct sock *meta_sk,
+					  struct sk_buff *skb,
+					  bool zero_wnd_test)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sock *sk;
+	bool force;
+
+	/* if there is only one subflow, bypass the scheduling function */
+	if (mpcb->cnt_subflows == 1) {
+		sk = (struct sock *)mpcb->connection_list;
+		if (!mptcp_is_available(sk, skb, zero_wnd_test))
+			sk = NULL;
+		return sk;
+	}
+
+	/* Answer data_fin on same subflow!!! */
+	if (meta_sk->sk_shutdown & RCV_SHUTDOWN &&
+	    skb && mptcp_is_data_fin(skb)) {
+		mptcp_for_each_sk(mpcb, sk) {
+			if (tcp_sk(sk)->mptcp->path_index == mpcb->dfin_path_index &&
+			    mptcp_is_available(sk, skb, zero_wnd_test))
+				return sk;
+		}
+	}
+
+	/* Find the best subflow */
+	sk = get_subflow_from_selectors(mpcb, skb, &subflow_is_active,
+					zero_wnd_test, &force);
+	if (force)
+		/* one unused active sk or one NULL sk when there is at least
+		 * one temporally unavailable unused active sk
+		 */
+		return sk;
+
+	sk = get_subflow_from_selectors(mpcb, skb, &subflow_is_backup,
+					zero_wnd_test, &force);
+	if (!force)
+		/* one used backup sk or one NULL sk where there is no one
+		 * temporally unavailable unused backup sk
+		 *
+		 * the skb passed through all the available active and backups
+		 * sks, so clean the path mask
+		 */
+		TCP_SKB_CB(skb)->path_mask = 0;
+	return sk;
+}
+
+/* Modified __mptcp_next_segment from mptcp_sched.c to re-send skbs through
+ * other paths
+ */
+static struct sk_buff *__redundant_next_segment(struct sock *meta_sk, int *reinject)
+{
+	struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+	struct sk_buff *skb = NULL;
+
+begin:
+	*reinject = 0;
+
+	/* If we are in fallback-mode, just take from the meta-send-queue */
+	if (mpcb->infinite_mapping_snd || mpcb->send_infinite_mapping)
+		return tcp_send_head(meta_sk);
+
+	skb = skb_peek(&mpcb->reinject_queue);
+
+	if (skb) {
+		// Reinjected or Redundant skb
+		*reinject = 1;
+
+		if (TCP_SKB_CB(skb)->dss[1] == 1) {
+			// Additional checks for a redundant skb
+			struct sock *subsk = get_available_subflow(meta_sk,
+								   skb,
+								   false);
+			struct tcp_sock *tp;
+
+			if (!subsk) {
+				pr_debug("redundant skb: deleted because of "
+					 "no-path\n");
+				skb_unlink(skb, &mpcb->reinject_queue);
+				__kfree_skb(skb);
+				goto begin;
+			}
+
+			tp = tcp_sk(subsk);
+			if (TCP_SKB_CB(skb)->path_mask == 0 ||
+			    TCP_SKB_CB(skb)->path_mask &
+			    mptcp_pi_to_flag(tp->mptcp->path_index)) {
+				pr_debug("redundant skb: deleted because of "
+					 "no-desired-path (provided path %u, "
+					 "wanted by path_mask %u)\n",
+					 tp->mptcp->path_index,
+					 (-1u ^ TCP_SKB_CB(skb)->path_mask));
+				skb_unlink(skb, &mpcb->reinject_queue);
+				__kfree_skb(skb);
+				goto begin;
+			}
+
+			pr_debug("redundant skb: passed (provided path %u, "
+				 "wanted by path-mask %u)\n",
+				 tp->mptcp->path_index,
+				 (-1u ^ TCP_SKB_CB(skb)->path_mask));
+
+		}
+	} else {
+		// Normal skb
+		skb = tcp_send_head(meta_sk);
+
+		if (!skb && meta_sk->sk_socket &&
+		    test_bit(SOCK_NOSPACE, &meta_sk->sk_socket->flags) &&
+		    sk_stream_wspace(meta_sk) < sk_stream_min_wspace(meta_sk)) {
+			struct sock *subsk = get_available_subflow(meta_sk, NULL,
+								   false);
+			if (!subsk)
+				return NULL;
+
+			skb = mptcp_rcv_buf_optimization(subsk, 0);
+			if (skb)
+				*reinject = -1;
+		}
+	}
+	return skb;
+}
+
+/* Modified mptcp_next_segment from mptcp_sched.c to re-send skbs through other paths */
+static struct sk_buff *redundant_next_segment(struct sock *meta_sk,
+					     int *reinject,
+					     struct sock **subsk,
+					     unsigned int *limit)
+{
+	struct sk_buff *skb = __redundant_next_segment(meta_sk, reinject);
+	unsigned int mss_now;
+	struct tcp_sock *subtp;
+	u16 gso_max_segs;
+	u32 max_len, max_segs, window, needed;
+
+	/* As we set it, we have to reset it as well. */
+	*limit = 0;
+
+	if (!skb)
+		return NULL;
+
+	*subsk = get_available_subflow(meta_sk, skb, false);
+	if (!*subsk)
+		return NULL;
+
+	subtp = tcp_sk(*subsk);
+	mss_now = tcp_current_mss(*subsk);
+
+	if (!*reinject && unlikely(!tcp_snd_wnd_test(tcp_sk(meta_sk), skb, mss_now))) {
+		skb = mptcp_rcv_buf_optimization(*subsk, 1);
+		if (skb)
+			*reinject = -1;
+		else
+			return NULL;
+	}
+
+	/* Redundant mechanism.
+	 * Only for non-reinjected/non-redundant skbs and for skbs that are
+	 * going through active sk.
+	 * Inspired in __mptcp_reinject_data() of mptcp_output.c file
+	 */
+	pr_debug("skb: skb on path %u\n", subtp->mptcp->path_index);
+	if (subflow_is_active(subtp) &&
+	    (!*reinject || (*reinject && TCP_SKB_CB(skb)->dss[1] != 1 ))) {
+		const struct tcp_sock *meta_tp = tcp_sk(meta_sk);
+		struct mptcp_cb *mpcb = tcp_sk(meta_sk)->mpcb;
+		struct sock *sk;
+		pr_debug("redundantable skb: %s skb on active path\n",
+			 (*reinject) ? "reinjected" : "new");
+		mptcp_for_each_sk(mpcb, sk) {
+			struct tcp_sock *tp = tcp_sk(sk);
+			struct sk_buff *copy_skb;
+			if ((sk != *subsk) && subflow_is_active(tp)) {
+				/* This is an additional active sk!! */
+				copy_skb = pskb_copy_for_clone(skb, GFP_ATOMIC);
+				if (unlikely(!copy_skb))
+					continue;
+				copy_skb->sk = meta_sk;
+				if (!after(TCP_SKB_CB(copy_skb)->end_seq, meta_tp->snd_una)) {
+					__kfree_skb(copy_skb);
+					break;
+				}
+				memset(TCP_SKB_CB(copy_skb)->dss, 0 , mptcp_dss_len);
+				/* Set the path_mask for this copy_skb blocking
+				 * all the other active paths...
+				 */
+				TCP_SKB_CB(copy_skb)->path_mask = mptcp_pi_to_flag(tp->mptcp->path_index);
+				TCP_SKB_CB(copy_skb)->path_mask ^= -1u;
+				/* Set one to mark this packet as a redundant
+				 * one and not a normal reinjection
+				 */
+				TCP_SKB_CB(copy_skb)->dss[1] = 1;
+				/* Enqueue */
+				skb_queue_tail(&mpcb->reinject_queue, copy_skb);
+				pr_debug("redundant skb: redundant skb scheduled for the path %u with path_mask %u\n",
+					 tp->mptcp->path_index, TCP_SKB_CB(copy_skb)->path_mask);
+			}
+		}
+	}
+
+	/* No splitting required, as we will only send one single segment */
+	if (skb->len <= mss_now)
+		return skb;
+
+	/* The following is similar to tcp_mss_split_point, but
+	 * we do not care about nagle, because we will anyways
+	 * use TCP_NAGLE_PUSH, which overrides this.
+	 *
+	 * So, we first limit according to the cwnd/gso-size and then according
+	 * to the subflow's window.
+	 */
+
+	gso_max_segs = (*subsk)->sk_gso_max_segs;
+	if (!gso_max_segs) /* No gso supported on the subflow's NIC */
+		gso_max_segs = 1;
+	max_segs = min_t(unsigned int, tcp_cwnd_test(subtp, skb), gso_max_segs);
+	if (!max_segs)
+		return NULL;
+
+	max_len = mss_now * max_segs;
+	window = tcp_wnd_end(subtp) - subtp->write_seq;
+
+	needed = min(skb->len, window);
+	if (max_len <= skb->len)
+		/* Take max_win, which is actually the cwnd/gso-size */
+		*limit = max_len;
+	else
+		/* Or, take the window */
+		*limit = needed;
+
+	return skb;
+}
+
+static struct mptcp_sched_ops mptcp_sched_redundant = {
+	.get_subflow = get_available_subflow,
+	.next_segment = redundant_next_segment,
+	.init = defsched_init,
+	.name = "redundant",
+	.owner = THIS_MODULE,
+};
+
+static int __init redundant_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct defsched_priv) > MPTCP_SCHED_SIZE);
+
+	if (mptcp_register_scheduler(&mptcp_sched_redundant))
+		return -1;
+
+	return 0;
+}
+
+static void __exit redundant_unregister(void)
+{
+	mptcp_unregister_scheduler(&mptcp_sched_redundant);
+}
+
+module_init(redundant_register);
+module_exit(redundant_unregister);
+
+MODULE_AUTHOR("Christian Pinedo");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("REDUNDANT MPTCP");
+MODULE_VERSION("0.89");
diff -ruN c/net/mptcp/mptcp_sched.c d/net/mptcp/mptcp_sched.c
--- c/net/mptcp/mptcp_sched.c	2015-08-04 16:50:26.991553981 +0900
+++ d/net/mptcp/mptcp_sched.c	2015-09-18 23:41:06.585277969 +0900
@@ -381,6 +381,7 @@
 	.get_subflow = get_available_subflow,
 	.next_segment = mptcp_next_segment,
 	.init = defsched_init,
+	.flags= MPTCP_SCHED_NON_RESTRICTED,
 	.name = "default",
 	.owner = THIS_MODULE,
 };
@@ -397,6 +398,7 @@
 	return NULL;
 }
 
+
 int mptcp_register_scheduler(struct mptcp_sched_ops *sched)
 {
 	int ret = 0;
@@ -456,6 +458,7 @@
 #endif
 
 	if (sched) {
+		sched->flags |= MPTCP_SCHED_NON_RESTRICTED;
 		list_move(&sched->list, &mptcp_sched_list);
 		ret = 0;
 	} else {
@@ -494,3 +497,130 @@
 	return mptcp_set_default_scheduler(CONFIG_DEFAULT_MPTCP_SCHED);
 }
 late_initcall(mptcp_scheduler_default);
+
+#if 1 //0917
+/* Build string with list of available scheduler values */
+void mptcp_get_available_scheduler(char *buf, size_t maxlen)
+{       
+	struct mptcp_sched_ops *sched;
+    size_t offs = 0;
+
+    rcu_read_lock();
+	list_for_each_entry_rcu(sched, &mptcp_sched_list, list) {
+        offs += snprintf(buf + offs, maxlen - offs,
+                 "%s%s",
+                 offs == 0 ? "" : " ", sched->name);
+    }        
+    rcu_read_unlock();
+}
+
+/* Built list of non-restricted scheduler values */
+void mptcp_get_allowed_scheduler(char *buf, size_t maxlen)
+{
+	struct mptcp_sched_ops *sched;
+	size_t offs = 0;
+
+	*buf = '\0';
+	rcu_read_lock();
+	list_for_each_entry_rcu(sched, &mptcp_sched_list, list) {
+		if (!(sched->flags & MPTCP_SCHED_NON_RESTRICTED))
+			continue;
+		offs += snprintf(buf + offs, maxlen - offs,
+				 "%s%s",
+                 offs == 0 ? "" : " ", sched->name);
+	}
+	rcu_read_unlock();
+}
+
+#if 1
+/* Change list of non-restricted scheduler */
+int mptcp_set_allowed_scheduler(char *val)
+{
+	struct mptcp_sched_ops *sched;
+	char *saved_clone, *clone, *name;
+	int ret = 0;
+
+	saved_clone = clone = kstrdup(val, GFP_USER);
+	if (!clone)
+		return -ENOMEM;
+
+	spin_lock(&mptcp_sched_list_lock);
+	/* pass 1 check for bad entries */
+	while ((name = strsep(&clone, " ")) && *name) {
+		sched = mptcp_sched_find(name);
+		if (!sched) {
+			ret = -ENOENT;
+			goto out;
+		}
+	}
+
+	/* pass 2 clear old values */
+	list_for_each_entry_rcu(sched, &mptcp_sched_list, list) 
+		sched->flags &= ~MPTCP_SCHED_NON_RESTRICTED;
+
+	/* pass 3 mark as allowed */
+	while ((name = strsep(&val, " ")) && *name) {
+		sched = mptcp_sched_find(name);
+		WARN_ON(!sched);
+		if (sched)
+			sched->flags |= MPTCP_SCHED_NON_RESTRICTED;
+	}
+out:
+	spin_unlock(&mptcp_sched_list_lock);
+	kfree(saved_clone);
+
+	return ret;
+}
+#endif
+
+int mptcp_set_scheduler(struct sock *sk, const char *name)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct mptcp_cb *mpcb;
+	struct mptcp_sched_ops *sched;
+    int err = 0;
+
+	if(!mptcp(tp))
+       	return -EPERM; 
+
+	mpcb = tp->mpcb;
+
+    rcu_read_lock();
+    sched = mptcp_sched_find(name);
+
+    /* no change asking for existing value */
+    if (sched == mpcb->sched_ops)
+        goto out;
+
+#ifdef CONFIG_MODULES
+    /* not found attempt to autoload module */
+    if (!sched && capable(CAP_NET_ADMIN)) {
+        rcu_read_unlock();
+        request_module("mptcp_%s", name);
+        rcu_read_lock();
+        sched = mptcp_sched_find(name);
+    }
+#endif
+    if (!sched)
+        err = -ENOENT;
+
+    else if (!((sched->flags & TCP_CONG_NON_RESTRICTED) ||
+           ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN)))
+        err = -EPERM;
+
+    else if (!try_module_get(sched->owner))
+        err = -EBUSY;
+
+    else {
+        mptcp_cleanup_scheduler(mpcb);
+        mpcb->sched_ops = sched;
+
+        if (sk->sk_state != TCP_CLOSE && mpcb->sched_ops->init)
+            mpcb->sched_ops->init(sk);
+    }
+ out:
+    rcu_read_unlock();
+    return err;
+}
+
+#endif
diff -ruN c/net/mptcp/mptcp_sm.c d/net/mptcp/mptcp_sm.c
--- c/net/mptcp/mptcp_sm.c	1970-01-01 09:00:00.000000000 +0900
+++ d/net/mptcp/mptcp_sm.c	2015-09-09 06:30:08.956421256 +0900
@@ -0,0 +1,373 @@
+#if 0
+#define MPTCP_SM_NAME_MAX 16
+struct mptcp_sm_ops {
+	struct list_head list;
+	void (*on_new_master_session)(struct mptcp_cb *mpcb,
+			                      struct sock *meta_sk);
+	void (*on_new_sub_session)   (struct mptcp_cb *mpcb,
+			                      struct sock *meta_sk);
+	void (*on_del_session)       (struct mptcp_cb *mpcb,
+			                      struct sock *meta_sk);
+	void (*on_destory_session)   (struct mptcp_cb *mpcb, 
+			                      struct sock *meta_sk);
+
+	char			name[MPTCP_PM_NAME_MAX];
+	struct module		*owner;
+};
+#endif
+
+#include <net/inet_common.h>
+#include <net/inet6_hashtables.h>
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#include <net/mptcp.h>
+#include <net/mptcp_v4.h>
+#if IS_ENABLED(CONFIG_IPV6)
+#include <net/ip6_route.h>
+#include <net/mptcp_v6.h>
+#endif
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/tcp_states.h>
+#include <net/transp_v6.h>
+#include <net/xfrm.h>
+
+#include <linux/cryptohash.h>
+#include <linux/kconfig.h>
+#include <linux/module.h>
+#include <linux/netpoll.h>
+#include <linux/list.h>
+#include <linux/jhash.h>
+#include <linux/tcp.h>
+#include <linux/net.h>
+#include <linux/in.h>
+#include <linux/random.h>
+#include <linux/inetdevice.h>
+#include <linux/workqueue.h>
+#include <linux/atomic.h>
+#include <linux/sysctl.h>
+
+#include <linux/module.h>
+#include <net/mptcp.h>
+
+static DEFINE_SPINLOCK(mptcp_sm_list_lock);
+static LIST_HEAD(mptcp_sm_list);
+
+//static void mptcp_sm_nothing(struct mptcp_cb *mpcb, struct sock *sk){}
+/* ------------------------------------------------------------------------*/
+static void sm_on_new_mas_sess(struct mptcp_cb *mpcb, struct sock *sk)
+{
+struct tcp_sock *tp = tcp_sk(sk);
+
+	if(sk->sk_family == AF_INET)
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI4:%d dst_addr:%pI4:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows, 
+                 &((struct inet_sock *)tp)->inet_saddr, ntohs(((struct inet_sock *)tp)->inet_sport),
+                 &((struct inet_sock *)tp)->inet_daddr, ntohs(((struct inet_sock *)tp)->inet_dport));
+	}
+#if IS_ENABLED(CONFIG_IPV6)
+	else
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI6:%d dst_addr:%pI6:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows,
+					&inet6_sk((const struct sock *)sk)->saddr, 
+					ntohs(((struct inet_sock *)tp)->inet_sport),
+                    &sk->sk_v6_daddr, 
+					ntohs(((struct inet_sock *)tp)->inet_dport));
+	}
+#endif
+
+}
+/* ------------------------------------------------------------------------*/
+static void sm_on_new_sub_sess(struct mptcp_cb *mpcb, struct sock *sk)
+{
+struct tcp_sock *tp = tcp_sk(sk);
+
+	if(sk->sk_family == AF_INET)
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI4:%d dst_addr:%pI4:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows, 
+                 &((struct inet_sock *)tp)->inet_saddr, ntohs(((struct inet_sock *)tp)->inet_sport),
+                 &((struct inet_sock *)tp)->inet_daddr, ntohs(((struct inet_sock *)tp)->inet_dport));
+	}
+#if IS_ENABLED(CONFIG_IPV6)
+	else
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI6:%d dst_addr:%pI6:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows, 
+					&inet6_sk((const struct sock *)sk)->saddr, 
+					ntohs(((struct inet_sock *)tp)->inet_sport),
+                    &sk->sk_v6_daddr, 
+					ntohs(((struct inet_sock *)tp)->inet_dport));
+	}
+#endif
+
+}
+/* ------------------------------------------------------------------------*/
+static void sm_on_del_sess(struct mptcp_cb *mpcb, struct sock *sk)
+{
+
+struct tcp_sock *tp = tcp_sk(sk);
+
+	if (sk->sk_family == AF_INET)
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI4:%d dst_addr:%pI4:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows,
+                 &((struct inet_sock *)tp)->inet_saddr, ntohs(((struct inet_sock *)tp)->inet_sport),
+                 &((struct inet_sock *)tp)->inet_daddr, ntohs(((struct inet_sock *)tp)->inet_dport));
+
+	}
+#if IS_ENABLED(CONFIG_IPV6)
+	else
+	{
+		mptcp_sm_debug("SM[%20s][%#x] [pi:%d/%d] src_addr:%pI6:%d dst_addr:%pI6:%d\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, tp->mptcp->path_index, mpcb->cnt_subflows,
+					&inet6_sk(sk)->saddr, ntohs(((struct inet_sock *)tp)->inet_sport),
+                    &sk->sk_v6_daddr, ntohs(((struct inet_sock *)tp)->inet_dport));
+	}
+#endif
+
+	mptcp_sm_debug("SM[%20s][%#x]  Rx[%llu, %llu bytes] Tx[%llu, %llu bytes]\n",
+				__FUNCTION__,
+		    	 mpcb->mptcp_loc_token, 
+				 tp->mptcp->nc_stat->rxPkts, tp->mptcp->nc_stat->rxOctets,
+				 tp->mptcp->nc_stat->txPkts, tp->mptcp->nc_stat->txOctets);
+
+}
+/* ------------------------------------------------------------------------*/
+static void sm_on_destroy_sess(struct mptcp_cb *mpcb, struct sock *sk){
+
+	mptcp_sm_debug("SM[%20s][%#x] [cnt:%d]\n",
+				__FUNCTION__,
+				mpcb->mptcp_loc_token, mpcb->cnt_subflows);
+}
+
+/* ------------------------------------------------------------------------*/
+struct mptcp_sm_ops mptcp_sm_default = {
+#if 0
+	.on_new_master_session = mptcp_sm_nothing, /* We do not nothing */
+	.on_new_sub_session = mptcp_sm_nothing, /* We do not nothing */
+	.on_del_session = mptcp_sm_nothing, /* We do not nothing */
+	.on_destory_session = mptcp_sm_nothing, /* We do not nothing */
+#else
+	.on_new_master_session = sm_on_new_mas_sess, 
+	.on_new_sub_session    = sm_on_new_sub_sess, 
+	.on_del_session        = sm_on_del_sess, 
+	.on_destory_session    = sm_on_destroy_sess, 
+#endif
+	.name = "default",
+	.owner = THIS_MODULE,
+};
+
+
+/* ------------------------------------------------------------------------*/
+static struct mptcp_sm_ops *mptcp_sm_find(const char *name)
+{
+	struct mptcp_sm_ops *e;
+
+	list_for_each_entry_rcu(e, &mptcp_sm_list, list) {
+		if (strcmp(e->name, name) == 0)
+			return e;
+	}
+	return NULL;
+}
+
+/* ------------------------------------------------------------------------*/
+int mptcp_register_session_monitor(struct mptcp_sm_ops *sm)
+{
+	int ret = 0;
+
+	if (!sm->on_new_master_session ||
+	    !sm->on_new_sub_session ||
+	    !sm->on_del_session ||
+	    !sm->on_destory_session )
+		return -EINVAL;
+
+	spin_lock(&mptcp_sm_list_lock);
+	if (mptcp_sm_find(sm->name)) {
+		pr_notice("%s already registered\n", sm->name);
+		ret = -EEXIST;
+	} else {
+		list_add_tail_rcu(&sm->list, &mptcp_sm_list);
+		pr_info("%s registered\n", sm->name);
+	}
+	spin_unlock(&mptcp_sm_list_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(mptcp_register_session_monitor);
+
+/* ------------------------------------------------------------------------*/
+void mptcp_unregister_session_monitor(struct mptcp_sm_ops *sm)
+{
+	spin_lock(&mptcp_sm_list_lock);
+	list_del_rcu(&sm->list);
+	spin_unlock(&mptcp_sm_list_lock);
+}
+EXPORT_SYMBOL_GPL(mptcp_unregister_session_monitor);
+
+/* ------------------------------------------------------------------------*/
+void mptcp_get_default_session_monitor(char *name)
+{
+	struct mptcp_sm_ops *sm;
+
+	BUG_ON(list_empty(&mptcp_sm_list));
+
+	rcu_read_lock();
+	sm = list_entry(mptcp_sm_list.next, struct mptcp_sm_ops, list);
+	strncpy(name, sm->name, MPTCP_SM_NAME_MAX);
+	rcu_read_unlock();
+}
+
+/* ------------------------------------------------------------------------*/
+int mptcp_set_default_session_monitor(const char *name)
+{
+	struct mptcp_sm_ops *sm;
+	int ret = -ENOENT;
+
+	spin_lock(&mptcp_sm_list_lock);
+	sm = mptcp_sm_find(name);
+#ifdef CONFIG_MODULES
+	if (!sm && capable(CAP_NET_ADMIN)) {
+		spin_unlock(&mptcp_sm_list_lock);
+
+		request_module("mptcp_%s", name);
+		spin_lock(&mptcp_sm_list_lock);
+		sm = mptcp_sm_find(name);
+	}
+#endif
+
+	if (sm) {
+		list_move(&sm->list, &mptcp_sm_list);
+		ret = 0;
+	} else {
+		pr_info("%s is not available\n", name);
+	}
+	spin_unlock(&mptcp_sm_list_lock);
+
+	return ret;
+}
+
+/* ------------------------------------------------------------------------*/
+void mptcp_init_session_monitor(struct mptcp_cb *mpcb)
+{
+	struct mptcp_sm_ops *sm;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(sm, &mptcp_sm_list, list) {
+		if (try_module_get(sm->owner)) {
+			mpcb->sm_ops = sm;
+			break;
+		}
+	}
+	rcu_read_unlock();
+}
+
+/* ------------------------------------------------------------------------*/
+/* Manage refcounts on socket close. */
+void mptcp_cleanup_session_monitor(struct mptcp_cb *mpcb)
+{
+	module_put(mpcb->sm_ops->owner);
+}
+
+/* ------------------------------------------------------------------------*/
+/* Fallback to the default session-monitor. */
+void mptcp_sm_fallback_default(struct mptcp_cb *mpcb)
+{
+	struct mptcp_sm_ops *sm;
+
+	mptcp_cleanup_session_monitor(mpcb);
+	sm = mptcp_sm_find("default");
+
+	/* Cannot fail - it's the default module */
+	try_module_get(sm->owner);
+	mpcb->sm_ops = sm;
+}
+EXPORT_SYMBOL_GPL(mptcp_sm_fallback_default);
+
+/* ------------------------------------------------------------------------*/
+/* Set default value from kernel configuration at bootup */
+static int __init mptcp_session_monitor_default(void)
+{
+#if 1 //0907
+	/* Just Test */
+    rSetRow(0,0x111dfaf,0xff11dfaf);
+    rSetRow(1,0x120dfaf,0xff20dfaf);
+    rSetRow(2,0x114dfaf,0xff14dfaf);
+    rSetRow(3,0x115dfaf,0xff15dfaf);
+    rSetRow(4,0x124dfaf,0xff24dfaf);
+    rSetRow(5,0x112dfaf,0xff12dfaf);
+    rSetRow(6,0x113dfaf,0xff13dfaf);
+    rSetRow(7,0x122dfaf,0xff22dfaf);
+    rSetRow(8,0x116dfaf,0xff16dfaf);
+    rSetRow(9,0x117dfaf,0xff17dfaf);
+    rSetRow(10,0x126dfaf,0xff26dfaf);
+    rSetRow(11,0x10adfaf,0xff0adfaf);
+    rSetRow(12,0x10bdfaf,0xff0bdfaf);
+    rSetRow(13,0x11adfaf,0xff1adfaf);
+    rSetRow(14,0x11bdfaf,0xff1bdfaf);
+    rSetRow(15,0x10edfaf,0xff0edfaf);
+    rSetRow(16,0x10fdfaf,0xff0fdfaf);
+    rSetRow(17,0x11edfaf,0xff1edfaf);
+    rSetRow(18,0x11fdfaf,0xff1fdfaf);
+    rSetRow(19,0x102dfaf,0xff02dfaf);
+    rSetRow(20,0x103dfaf,0xff03dfaf);
+    rSetRow(21,0x130dfaf,0xff30dfaf);
+    rSetRow(22,0x131dfaf,0xff31dfaf);
+    rSetRow(23,0x12cdfaf,0xff2cdfaf);
+    rSetRow(24,0x12ddfaf,0xff2ddfaf);
+    rSetRow(25,0x134466e,0xff34466e);
+    rSetRow(26,0x135466e,0xff35466e);
+    rSetRow(27,0x138466e,0xff38466e);
+    rSetRow(28,0x139466e,0xff39466e);
+    rSetRow(29,0x136466e,0xff36466e);
+    rSetRow(30,0x137466e,0xff37466e);
+    rSetRow(31,0x13a466e,0xff3a466e);
+    rSetRow(32,0x13b466e,0xff3b466e);
+    rSetRow(33,0x12e466e,0xff2e466e);
+    rSetRow(34,0x12f466e,0xff2f466e);
+    rSetRow(35,0x132466e,0xff32466e);
+    rSetRow(36,0x133466e,0xff33466e);
+    rSetRow(37,0x11a466e,0xff1a466e);
+    rSetRow(38,0x11b466e,0xff1b466e);
+    rSetRow(39,0x10e466e,0xff0e466e);
+    rSetRow(40,0x10f466e,0xff0f466e);
+    rSetRow(41,0x1340727,0xff340727);
+    rSetRow(42,0x1350727,0xff350727);
+    rSetRow(43,0x1380727,0xff380727);
+    rSetRow(44,0x1390727,0xff390727);
+    rSetRow(45,0x1360727,0xff360727);
+    rSetRow(46,0x1370727,0xff370727);
+    rSetRow(47,0x13a0727,0xff3a0727);
+    rSetRow(48,0x13b0727,0xff3b0727);
+    rSetRow(49,0x12e0727,0xff2e0727);
+    rSetRow(50,0x12f0727,0xff2f0727);
+    rSetRow(51,0x1320727,0xff320727);
+    rSetRow(52,0x1330727,0xff330727);
+    rSetRow(53,0x1120727,0xff120727);
+    rSetRow(54,0x1130727,0xff130727);
+    rSetRow(55,0x10e0727,0xff0e0727);
+    rSetRow(56,0x144f6d3,0xff44f6d3);
+    rSetRow(57,0x145f6d3,0xff45f6d3);
+    rSetRow(58,0x10f0727,0xff0f0727);
+    rSetRow(59,0x21dfaf,0xff21dfaf);
+    rSetRow(60,0x25dfaf,0xff25dfaf);
+    rSetRow(61,0x110dfaf,0xff10dfaf);
+    rSetRow(62,0x815ba8c0,0x845ba8c0);
+#if 1 //for office
+	rSetRow(63,0x8f14a8c0,0x8f14a8c0);
+	rSetRow(64,0x7d14a8c0,0x7d14a8c0);
+	rSetRow(65,0x8714a8c0,0x8714a8c0);
+#endif
+#endif
+
+	return mptcp_set_default_session_monitor("default");
+}
+late_initcall(mptcp_session_monitor_default);
